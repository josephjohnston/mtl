

Pipelines dictate user specified functions, the shaders, and how they will operate.
these are specified using pipline state objects. the objects are non-transient and are made to be used repeatedly across passes.

DESCRIPTOR
you can reuse a descriptor for creating different pipeline state objects.

describes compute function, threads, 


OPTION

we also use pipeline options to create a pipeline state object.

The option, an enum, gives info about whether to provide 

MTLArgument is depreciated with replacement MTLBinding

so the middle two options, ArgumentInfo and BufferTypeInfo, which relate to MTLArgument
The first option, None, says no reflection data.
The last option, FailOnBinaryArchiveMiss, says to fail creating the state if the compiled shader is not in 'the' (which?) binary archive.

REFLECTION

MTLAutoreleasedComputePipelineReflection
this is a type alias, for a MTLComputePipelineReflection that is autoreleased
MTLComputePipelineReflection is a class that gives info about arguments to the shader. 
we pass the pipeline creator a pointer to an empty instance, and if successful fills the instance. 
this class has a single item, that is the readonly bindings property. which is an NSArray of MTLBinding's. 

bindings are for 16.0+, while prior uses MTLArguments

STATE



Apple Silicon Stuff I THINK

use TBDR (tile-based deferred rendering), in contrast to IM (immediate mode)
render destination is broken into grids. 
each tile is processed on a core.
first does geometry, then renders by shading only visible primitives.
new features starting with A11: imageblocks, tile shading, raster order groups
it generates `potential' pixels for the remaining visible primitives, it feeds to a fragment shader, and writes the results to tile memory. only later it might write to device memory. 
do tile shaders read from tile memory too? 

they're probably just using threadgroup memory for tile memory, as rendering is very parallel. 

imageblocks are tiles of image data in local memory. 
it can pass data from the fragment to the tile shader.

threadgroup memory is best for unstructured data, imageblocks are better for image data

only implement what we need.
read about the methods, understand their functions, then choose only a few. 
but there's so much to understand. we need to be able to divide the domain. we can't learn about everything trying to pack tgoether all information and only design then. 





LANGUAGE

we need to think of the functionaity we want to expose. 
create a device, create queues, 
are we passing back variables? if not, how to identify? we need to identify if they are to be queried dynamically. 
i was hoping for a fully delcarative interface, nothing imperative. well, we can have like reduc where output is indeed declarative, and yet can reason arbitrarily in transition. 
so what is this state, and what is the event input? 
what about hooks? 
one kind of event is a callback, but we're not supposed to use too much on these, and if so doing so another thread. but with our updates we don't know if they'll be long or short. but i guess during the eval we could see if they're long and if so we delegate to another thread.
so i guess the pattern is we just specify shapes 
i think everything is a reducer. most of time it should amortize and return the same. 

our job is to read the state and apply changes necessary.
remember a huge benefit of this architexture is debugging. 
instead of allowing arbitrary state we can architect it to ensure fast updates. so we know exactly where to check for updates. 
i think we can forget about multithreading now and still have a good transition later. 

but immutable data structures might be costly. maybe this won't work. how else, then to present an interface? maybe we cant. 


command buffer descriptor
    only retained references and error option
use it to create a command buffer from the command queue
now with command buffer we can create encoders


of course we need to build this incrementally. 
lets start with 

what kind of framework do we want?

i have a device and a queue.
we encode commands into the queue. but we do so in blocks of command buffers.
in each buffer we use multiple encoders. 
an encoder can only do one pass 

we need to learn about resource heaps and argument buffers, and what workflow looks lik with them, and what they replace.


we could write rust code that emits metal code in order to unify shaders and encoders.





