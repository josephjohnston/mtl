
branch divergence
memory bank conflicts
inactive threads
synching
instruction overhead


represent algos as trees (maybe multiple outputs)
T_1 = node_count
T_inf = depth
p processors takes time T_p with
T_1/p <= T_p <= T_1/p + T_inf

imageblocks
or maybe we start with naive solution
initial algorithm. naive.


we'll try first with n=2^6, so degree = 32. fully splits.
0: 2^5
    0: 2^10
1: 2^15
        0: 2^20
2: 2^25
    1: 2^30
3: 2^35
            0: 2^40
4: 2^45
    2: 2^50
5: 2^55
        1: 2^60
6: 2^65
    3: 2^70
7: 2^75

what about when it no longer is divisible by 2.
can we consider next divisible, eg 7 for 49. I think
what about when m is odd? we still have C[m,1] mod n.
m = 31. n = 10.
it splits in two. have one poly as zeta is the primitive 2'th ROU. that's -1. what's -1 here?
2^15. then it splits in two. so what are the zeta's? ord_n(p) = 2. 

We have a prime p = C[31,2]. ord_n(p) = 2.
we can take any m = 2. then we have X^{n/m}-z = X^{n/2}-z where z is a primitive 2'th ROU, that's -1.
X^{n/2}+1. there's always a primitive 2'th ROU, otherwise p=2|n.
now when p=C[k,2], we have 2^{k/2}=-1. because 2^{k/2}^2 = 2^k = 1 since 2 is a primitive k'th ROU.
but what about when k is odd? take k=31. then -1 is 2^{15}*sqrt{2}. sqrt{2}=2^16, as 2^32=2^31*2
but 2^15*2^16 is actually 2^31 = 1.
of course -1 = p-1 = 2(2^30-1)
we can ake larger m until we know its irreducible. and there should still be primitive m'th ROUs. still binary tree form, but we don't know what those roots are. 


Phi_n becomes X^2-zeta where zeta is a primitive 
now suppose order was 1, not 2. then m = n.


start with X^32-2^40
split into (X^16-2^60)(X^16-2^20)
we'll end up with like (X^4-2^5)
we'll need to figure out the primitive 16'th ROUs to get quadratic.

we want to copy first to local memory. 
how much will fit in local memory? 1024 * 32 bytes. wow, that's a full ring element.

copy to local memory.
now split in two. multiply and add. 
this is with each warp. 

realistically, how would we want to do this?
how can you break the problem into parts?
you can't. only with modulus crt. on the other hand, you can with FFT if you start from the bottom up. you can 

consider H. memory can hold 32*8 bits, .. ie as many as 8 ring elements. or 32 commit elements

what does a single warp do?
suppose already down to size of warp.
so warp must do CRT on its own size. what should the threads do?


i don't understand putting address specifiers when multiplying. has to do with uniformity. 
if we

to operate on a value in shared memory, does it first have to be read into a register?

we can put what we can in the class. to make it reusable.

here suppose each thread takes care of 1 when folding in half. 

maybe ther's no use for warps.
just think in threads. suppose same number of threads as elements.
we fold in half. 
we want each thread to take care of one mult. the number of mults is always N/2.
ok, to fold size N, we use N/2 threads. each focuses on both parts, one mult and one add.
we read the two elements from memory. we need to since we're operating on them. apparently things go into registers anyway.
multiply all by the same number, and hence we can do that multiplication using shifts and such. then we add and subtract and store back in whatever order. 
memory access is important.
so how to generalize this beyond the top fold?
for a quarter fold. again each thread takes care of two. but the threads here have to sync, because each thread is now operating on at least one value it didn't write, and at least on

seems we're on to the general CRT algo, though not yet the multplication.
within a warp, we won't write and sync but use subgroup ops.

for now, focusing on a single block, we can forget about how data is loaded to shared memory. 

generalize to where each thread handles more than 1 output.
so if it handles k outputs, it reads 2k elements from memory.
this is very ordered.
still doing level 1 fold, this means accessing k bottom and k top elements, both sets sequential. but now we're accessing more elements than banks. i think there's a k-way conflict. so we see if it saves anything.
for now just try this top level.


1. each thread takes care of 2. offset in the middle, by 32.
we have each warp processing size 128.
we can double the threads or blocks while doubling the input size. but then we need to make sure each block and warp target correctly.

input is 4 times larger than thread count.
how to map?
first map blocks. bid maps to input bid*4. then independent of warps, each thread maps to input offset tid*4.

Model 1:
Input length N. Threat count T. T(2K) = N.
Doing a 1-fold.
    K=1:
        Thread count 2^10:
            N=2^27: 139
            N=2^28: 274
            N=2^29: 538
            N=2^30: 1061
        Thread count 2^9:
            N=2^27: 137
            N=2^28: 275
            N=2^29: 535
            N=2^30: 1050
        Thread count 2^8:
            N=2^30: 1068
        Thread count 2^5:
            N=2^30: 1051

        Switching now from i8 commits to u8, and times become:
        Thread count 2^10:
            N=2^28: 164
            N=2^30: 622
        Thread count 2^5:
            N=2^30: 621

        Switching now to adding P before taking modulo. So apparently the arithmetic matters a lot. 
        Thread count 2^10:
            N=2^28: 15
            N=2^30: 64
        Thread count 2^5:
            N=2^30: 77
    K=8. With continuous access
        Thread count 2^10:
            N=2^27: 16
            N=2^30: 73
        Thread count 2^5:
            N=2^30: 76
    K=8. With offset access
        Thread count 2^10:
            N=2^27: 8
            N=2^28: 16
            N=2^29: 32
        Thread count 2^5:
            N=2^28: 16
            N=2^29: 33
        N=2^30 seems to break somehow. also still seems right, and still reports around 32. I don't see what's changed. We can imagine it to be between 64 and 80.  
Doing a 2-fold:
    K=1:
        Thread count 2^10:
            N=2^27: 13
            N=2^29: 38
            N=2^30: 75
        Thread count 2^5:
            N=2^27: 20
            N=2^28: 25
            N=2^30: 69
    Interesting how this one is slightly slower. Before I made the mistake of making N half as large as it needed to be and this at this drove up time by like 16 times.
So of course we can generalize to T-fold and K-fold.

Model 1 just does a single fold. To go to multiple folds, we can't just read and write to global memory. 
we need to start using shared memory and subgroups.
we'll still leave alone the number system until last. 
we'll also leave alone the multiplication, second to last.
so far we're warp-independent. just take an array of threads which processes an array of elements, each thread processing multiple. 
we'll do as much as we can in a warp to make use of subgroups and avoid synchronization. we'll probably have 32 warps. each can handle an independent block, like a whole ring element. we already have this independence. each thread in a warp calculates it's outputs, 
you need synchronization whenever you're going to read/write.
part of the time we'll need synchronization, part of the time not when warps are on their own. looks like it's the latter half where results are independent. in the first half, warps can compute 
think of threads independently. then suppose two threads can exchange their data. 

the nature of the pattern seems to be that you have two places in offset. you read them and write back to those places. so one all thread do this, we split in half, and for the next round you need 

with 1024 by 32 bit ring, we can support 8 ring elements. the fact that inputs are small doesn't seem to help cuz they quickly grow on FFT. only helps if we like we load 32 for just one round and write back to global. we won't do that. wow, i didn't expect this, so the small inputs really give us nothing except their small global mem storage. 
i guess this is where we weigh options of lattice vs modulus size. if we halve lattice size then we'll have to double the time, but that may be better than a 1024 lattice that's hard to fit.
how many elements to commit? 2^10*2^5 / 2^10*2^3 = 2^2 = 4. so maybe, hopefully we can fit exactly this many. fuck. we need 8 of them.. unles we commit in 16 bits. and remember we can go high on bits most of the time. i think this includes children inputs. 

can we really process in tree form completely separately?
consider regular form. we take and input in full modulus form. we take bit decomposition. commit to that. for collision resistance, we note that by committing to bit decomposition we've committed to the outputs. its homomorphic because you have a tree of nodes.
so the qustion is, can you commit the bit decomposition in CRT form? yes, because that's a ring ISO.


gpu algo:
suppose we have a black box algo that splits.
lets see how we want to make the algo abstractly, then design subroutines. 
each thread reads elements from global memory, and i guess they perform the full first split.
maybe we can structure them so by reading multiple they can perform the second split as well. yes we can. luckily this is the memory access pattern we did for K with offset.
when going this offset way, and performing reads in sequence, they are adjacent. 
when going the adjacent way per thread, they are not in sequence in space but in time.
apparently, the way GPUs work, a single thread cannot read multiple at the same time. only the separate threads can.
so we want to prioritize, threads in a warp reading (at same time) to adajcent locations. we can experment with how much each can read, maybe more than 32 bits. 
there are multiple memory reads per thread. we want to first read what we'll multiply so we can do that while the other loads. it was just that we couldn't get that example to go to 30 bits. it erred somehow. 
interesting how putting N in constant memory rather than passing as arg made it slower. 
once we finish this first round, which I'm not sure how it's bound, I guess by registers, threads store results to shared memory, and warps can do it in any order, well actually threads can too just make sure no bank conflicts. 
now actually we can go further. before writing to shared memory, threads in a warp can share among each other. so whereas they'd usually write 32 values to memory, they now combine for the processed 32 values. but whereas before each thread handled multiple, now its 32 threads handling 32 values. if K = 16 then we can get 16*2*32 = 1024 elements so a whole commit per warp. the question is register space since need all in place to go to next round. so let's max out the register space. this means fewer warps, but probably better used, we'll see. 
so i think each warp is acting to begin as we already have it. accessing many at offsets and computing a fold. from which it can then compute more folds. we'll write subroutines for these. then it gets down to where we'll share between threads in a warp. note the karatsuba only comes later when multiplying. maybe while doing the arithmetic after the initial loads, it could be simultaneously loading the constants into shared memory because so far we won't need shared memory. 
we can improve by noting the inputs are small, 8 or 16 bits, so a thread can read multiple at a time. so this is the continuous model but all at the same time. 
we must sync, because next one must read values that were stored by multiple wa
where to begin? lets make subroutines. use templates.
1. memory access into registers. each thread in each warp has several continuous elements.
let's just try this and see how it performs depending on how many element each thread accesses.
    index by threadgroup size, all with N=29
        uchar: 32, ushort: 127, uint: 281
        maybe this is due to arithmetic?
        partly: without arithmetic we get
        uchar: 32, ushort: 35: uint: 46
    with arithmetic but no reduction:
        uchar: 31, ushort: 35, uint: 47
        so same as with no arithmetic. put in the modulus and arithmetic takes more time so it bumps up. 
        why should it be any more with uint with no arithmetic than with uchar? it must be about the memory accesss.
    now lets try accessing continuous but for separate entries, so it's not an arithmetic issue.
        ucar2: 18, ushort2: 23, uint2: 
lets start over only reading input, no arithmetic and no stores.

uint1: 47
ushort2: 35,
uchar8: 30
but these are all the same amount of memory. suppose instead we increase beyond 32 bits. lets try ushort4. 

we want to compare loading 32 bits per thread to loading 64 bits per thread. 
uchar: 16, ushort: 18, uint: 24, ulong: 37
here we're increasing the amount of memory, doubling every time, yet the access time doesn't double.
1: 19, 2: 15, 4: 13, 8: 13, 16: 13
so in the simplified example, it stays the same. 
so i guess we design it to allow for any number and then test again.
so we interpret input in some vector form. elements might be smaller than 32. we extract the elements and do arithmetic, but outputs are in 32. from there we'll have 32 bit values
one issue is we'd ideally have input in vector form with 32 bit elements, so we can overwrite it. what to do with the register space we have from reading the input? maybe just trust compiler to delete it. 

what is N? it's the size of problem we'll compute. but we want to support inputs of different lengths, all the way from 1 bit to 24 bits. small bits have optimizations especially in the first few rounds. 
eventually within a thread each value becomes 32 bits. therefore i guess there's no benefit to trying to compact it. so we'll take the inputs and expand it out into 32 bit registers.

lets figure out how many registers we have




\section{K}

We have an input of N elements.
We have T threads.
T*2*V = N.
Each thread will read V values from each half of N, each value offset by T = N/(2V)
Threads will proceed logically in parallel, warps physicall in parallel.
They will read from low indices of N to high indices.
At each step note that all T threads are accessing adjacent indices of N, in both the bottom and top half.
This step occurs V times.
Every time we load we can proceed with arithmetic on that load, while we load others.
To implement, we give each thread two arrays of length V, one for the top elements and one for the bottom elements.
Since there's V steps in N/2, stride is N/(V*2)
At step j in {0,..,V-1}, thread id loads bottom element id+stride*j and top element N/2+id+stride*j.
that's wrong. 

Benchmarking:
We'll try with N=2^29.
B = 2^5:
    V = 2^0: 31, 56, 31, 55
    V = 2^1: 55, 65, 32, 66
    V = 2^2: 31, 31, 55, 32
    V = 2^3: 32, 56, 32, 56
    V = 2^4: 38, 71, 39, 71
    V = 2^5: 87, 179, 86, 177
B = 2^10
    V = 2^0: 55, 31
    V = 2^1: 55, 32
    V = 2^2: 55, 31
    V = 2^3: 56, 32
    V = 2^4: fails
B = 2^9:
    V = 2^4: 37, 70, 36

Seems reasonable. Right now we're not doing any arithmetic, only identity copying.
Now we go to the next step, but that's just arithmetic.
We're done loading until we get to multiplication.
For now we're focused on completing the CRT.
Each of B threads has loaded 2*V elements out of N.
But how many per warp? 32*2*V, which has good performance for V=2^3, reasonable for V=2^4, so that's a total of 2^9 and 2^10 respectively.
Suppose a full ring element can't fit in a warp. A warp could still compute part of an element but that requires reading the whole input. To do so, we modify the reading top levels..
The lower levels within a thread are just arithmetic. At the end, each thread and fully reduced 2*V elements. So while we started with N parts, we now have N/(2*V) parts, which is the number of threads T, each part of size 2*V.
eg started with 1024 parts, now have 2^3*2 = 16 parts per each of the 32 threads of 2 warps, so a ring element in every 2 warps. Or we now have  2^4*2 = 32 parts per each of the 32 threads of 1 warp, so a ring element in each warp.
If we have 1024 parts to start, we'll reduce to 1024/8 = 128 parts. If each thread has 2*V parts, then it takes 128/2V threads to hold all parts. Eg V = 2^3, so it takes 8 threads. 
What's interesting here is when you fold, you need the offsets. But when you're at irreducibles you don't.

Concrete calculation:
Suppose we have N=1024, and irreducible of size 8, so there are 128 of them.
Suppose a single warp reads the whole thing, that is 32*2*V = 1024, that is V = 2^4.
Then I guess each thread ends up with each thread in a warp holding 4 irreducibles of size 8 to them multiply.
Meanwhile we've loaded the 2^10 constants, that's 2^10*2^2 bytes into shared memory. We'd do this for 8 warps, that's 2^{10+2+3} bytes, which is the limit. 
Then for multiplication, we do Karatsuba independently, per thread per each of the 8 irreducibles. Hopefully we can do this with overwriting existing registers cuz we won't have more. 
(a0+a1*x)(b0+b1*x) = (a0*b0) + [(a0+a1)(b0+b1)-(a0*b0+a1*b1)]x + (a1*b1)*x^2











simd we know has lanes, but each lane is just a data point, not a whole thread.







at the secon d level, each thread now continues for V levels.

may be can optimize additions for no overflows, since we add and subtract the same two values, so if there overflow there should be underflow and vice versa. 




we're not committing to 32 different ring elements. probably only like 8. so it's like 4 warps could cooperate for one. or cut the warp count if each warp can stay active.



NEON:
neon units operate on 128-bit registers register files
registers can be accessed as 8,16,32,64,128-bit registers.
registers contain vectors.
vector divided into lanes holding elements. all elements have same type.
128-bit vector contains: 16*8, 8*16, 4*32, 2*64
64-bit vector contains: 2*32, 4*16, 8*8
always 64 or 128-bit vectors

https://developer.arm.com/documentation/den0018/a/Introduction/Fundamentals-of-NEON-technology?lang=en


