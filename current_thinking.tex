reasons we pursued GPU:
    we thought it could well be done, why?
    risc0 said it improved 50x. MatterLabs used one, hearing of other projects too. Then there's ZPrize. 
how much time wasted?
    we've been planning on GPUs forever, probably a year, but we didn't get around to learning about GPUs in detail until November, and then only really programming it late November and December. The general learning I did on GPUs early november is still useful, and the programming of rust is still useful, though the objective-c and the bindings less so. since December I fooled around finally getting xcode to work with devices and rust, then I switched to number systems.
    so while if I had known of GPU being unfit for job, I'd have used time much better just on rust on cpu, what I spent time on isn't so irrelevant, especially if we end up using the GPU in some way as we'd like to for commits and proving hashes. 
what can we use GPU for?
    commits see obvious. not sure if we'd save them on the GPU for witness reduction or do that on the CPU. probably the CPU since they have to be in memory there anyway for arithmeization, and multiplying by challenges is just some additions. we have to stream over witnesses and get back commits. If we can't use GPU for this arithemtic reduction, I don't think it would be good for anything. 
    proving hashes. this is important. we absolutely need a performant way to perform hashes. can it be done with GPU? suppose we're able to isolate hashes to prove in bulk. we stream over pre-image vectors to the GPU. the idea is we can do isolated proofs since the statements are clear and isolated. 
        This should only be worth it if the commits are worth it. And any more general proofs on the GPU should only be worth it if these hash proofs are worth it. 
        So how would it work??
        I suppose first of all we would use the global scope commits already in place to commit to the whole pre-image vectors. But we'd also need to commit to intermediate states. That can be done by either shipping all those over too, or better somehow generating them.
        The more we can do in threadgroups with fewer global memory ops the better. 
        One option is only committing to pre-images and doing deterministic reductions to those commits in threadgroups.
        Obviously doing proofs in threadgroups will require challenges. These can be computed in threadgroups and then written to global mem so they themselves can be proven. 
        The output of each threadgroup should hopefully have reduced a series of committed-to hashes, which the threadgroup read from memory, to an eval on those hashes. In other words, threadgroups perform arithmetizations. It's the job of the threadgroup to do as many hashes with as little memory, taking advantage of the determinism. 
        It a threadgroup has to commit to intermediate states, I think that could be done locally, but then witness reduciotn would need to be local unless it wants to write out those values to global mem.
        We then have in global memory all commits and their pending evals. I think a series of eval reductions then must take place, again reading and writing from threadgroups.
        Then we have witness reduction. This will a third time require global reads and writes by threadgroups. 
    proving more general.
        the same as hashes, except now more general functions, still each threadgroup doing multiple instances of the same function, but now different threadgroups, or even warps, can do different functions. still must be able to generate, hopefully the case given algebraic logic. Only try this if Hashes work out.
but what about non-local GPU usage?
    Seems this is what the others were doing and I'm not sure it will work for us, especially because we want sophisticated arithmetization, which entails more reads and writes. 

What to do next?
    either we work out arithmetization and get to implementing on CPU, or we focus on commits and then hashes for GPU. 
    would be nice to get comfortable with gpu, referring to plonky2 and arkworks. also good to settle on arithmetization. but also good to have something completed with Metal.
    Maybe first could be implementing commits on both and comparing performance to get an initial idea if GPU will be at all useful. But I don't want to spend much time learning more Metal. But we already did both sides on basic reduction and this prob won't change much. It's a completely viable 
    issue is we already know gpu can do commits, but getting it right takes a lot of time. seems that can be delgated to others... in which case, what's the point of all i've learned about it?
    i can say, i put effort into building a bridge between rust and gpu, and learning about GPU, so I know it can be done.
    algo? 

    on considering how best to spend my time, there's using what knowledge I have, and there's what knowledge I need.
    I have zkp knowledge, and now this is best spent by settling on cpu arithmetization. there's also the cpu coding I need to learn. for gpu, I don't feel I should learn in that direction if we're not going to use it for more than commits and hashes. 
    we will next implement our commit kernel. 

