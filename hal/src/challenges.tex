


\section{Location/value}

Two parts to sampling.
We independently sample the locations and the values.
Location space is much bigger.

for values, just read directly, interpreting hash output segment as uniform.
for locations, use rejection sampling.

what if we do uniform. just generate T random values at random locations, and add them all up.
maybe try that first. 



\section{+-1}
we compare l2 norm of original commit, to l2 norm after multiplied by challenge.
we get mean values multiplicatively below sqrt{N}
we get std value just about sqrt{N}

I thought this would indeed be the case, but for all cases (here it's just std) rather than just N=2.
So we could choose eg 128-size ring with l2 growth 11.3, ie doubled that's 7 bits.

ok, so when they talk about std, maybe they mean across the challenge randomness as we've done here.



\section{std of uniform}
values uniformly distributed in an interval N
have std N/sqrt{12} around the N/2




if we have random a vector, we calculate std.
then we take dot product with another random vector.
is std of dot product across the other random vector, S*sqrt{L} or S*T where T is the std of the sum?
or is it S*V where V is the std of the sum of challenge coefs across challenges?
or where V is std of l2 values, or std of squared l2 values.
for us so far it seems to be S*V, or one of the others also close.
we'll probably just end up doing our own heuristics.

so it seems that you can approximate the std of the dot products with the std of the commit and the std of the sum of the challenges (which is l2 when the challenge vector entries only change positions or sign)

so we've go z coefs.
we can approximate the std by product of std of commit with the other possibilites we saw above, eg with std of sum of challenge coefs over challenges.
we can take the bottom b digits of the result. for small b it's uniform, as it gets larger we see the gaussian.
we could split so they have same std.

our real concern is now not the aggregation, but the cross multiplication. while aggregation is average-case, cross multiplication is worst-case. eg if we use operator norm, we need a good way for verifier to check it.

oh, in addition to multiplying by std of sums, we multiply by sqrt of r (number of vectors amortized)



|c|^2 = sum_{i=1}^n M_i^2 = variance of sum of coefs
where M_i is a random challenge.
so you could take any vector, and as long as the values are pre-determined, the signs can be random and the locations can be random. 
can be re-phrased as l2 norm of challenge equals the std of sum of coefs.
we saw this in the +-1. l2 norm was sqrt{N}. the growth factor was also sqrt{N}. does this mean the growth factor is the variance of the sum of coefs?
how is this useful?
this occurs because cross terms average to 0 since challenge coefs are signed, and we divide by number of trials we do.






