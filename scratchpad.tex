

now for CRT
first on CPU

lets try cyclo n = 1024 with H
lets first figure out the ROUs

2^80 = 1
2^40 = -1, order 2
sqrt{-1} = 2^20, order 4
2^10, order 8
2^5, order 16


we want to use all of them.
there's 80 of them, the first 80 powers of 2.
which ones do we need?
n = 1024
ord_n(p) = 4
m = n/ord_n(p) = 2^10/2^2 = 2^8 = 256

we need the 128 primitive 256'th roots of unity.

max divisible by 2 is 80/40/20/10/5

so how do we find the 256'th ROUs?

we look at the multiplicative group.
2^8*...
subgroup 2^8 is these 256 ROUs
now what are the elements?
we just need to find 1 generator.
we'd like 2, but that generates group order 80, which doesn't divide 256
oh no, 2^5 generates order 16, which divides 256. but it's not primitive. 
so maybe these 2-powers will only serve for arithmetization.
here we need elements of order 256.
so how do we find them?

since n is a power of 2, we can split in 2 each round.
m=2
    1 monic factor of degree n/2
    0: 2^40
m=4
    2 monic factors of degrees n/4
    0: 2^20
    1: 2^60
m=8
    4 monic factors of degrees n/8,
    0: 2^10
    1: 2^30
    2: 2^50
    3: 2^70
m=16
    8 monic factors of degrees n/16
    0: 2^5
    1: 2^15
    2: 2^25
    3: 2^35
    4: 2^45
    5: 2^55
    6: 2^65
    7: 2^75

at each step we take the poly and split in half, cuz we have X^{m/2} = zeta
we're in power form, we split in half, multily top half by something different and add.
what are the cosets?
all of them will be 

start with X^{n/2}+1
split into factors
X^{n/4}-2^20, X^{n/4}-2^60
we multiply tops by 2^20, then add and subtract from bottom.

split each into two factors
one is now 2^50, the other 2^70
are these inverse?
no, but suppose we shift out 30. then we get 2^0 and 2^60
maybe from here down we need two multiplications. but we can do it with a common shift of 50. then 20 is left for one.
first lets just try full multiplications.

to make this work we need to multiply by powers of 2. 
how high can we go with H?




0: 2^5
    0: 2^10
1: 2^15
        0: 2^20
2: 2^25
    1: 2^30
3: 2^35
            0: 2^40
4: 2^45
    2: 2^50
5: 2^55
        1: 2^60
6: 2^65
    3: 2^70
7: 2^75

http://dsp-book.narod.ru/FFTBB/0270_PDF_C03.pdf

those that differ by 40 are inverses, and go together.

maybe it's always of the form X^{n/m}+-zeta^{n/m}
this is worth documenting. when does this occur?


apparently you can lay the data out in a grid, then do FFTs in each dimension. of course.

when doing an FFT and doing all roots of unity, maybe we can fit in another layer.
actually there we can just do size 80 FFT, whereas here w're doing size 8 CRT.

first suppose using the full n ROU's generated by 2, a subset is the primitives of those.
then p = Cyclo_n(2)
n need not be a power of 2.

p = Cyclo_n'(2)
here 2 generates n' roots of unity. of those, phi(n') will be primitive. we have
phi(n') = phi(m) = phi(n/ord_n(p))
so we don't need n'=m, we just need phi(n')=phi(m)


what if p is composite?
then we have Phi_n modulo 2 primes. 

we'll do virtual left shifts. 
they will always be by multiples of 5, then 10, then 20.
how to reduce this. review reduction
we split into top and bottom.
bottom becomes low, 32 bits
top becomes mid and high, 8 and 24 bits
we subtract high from low to get diff, 32 bits.
we multiply mid by eps to get prod, 32 bits.
we add diff and prod.

today we want to implement CRT.
we want as little reliance on number system as possible.
so lets use a library to start. 
we implement on CPU.

ring n=2^1024.
p = H.
take an element, split in two, multiply by 4th ROU

maybe we could do it in place, not copying vectors but modifing them.
saves memory, prover just holds the top layer.

now how do we check our answers?
we could try undoing CRT and compare with poly mult.
how to invert CRT.

they're doing inverted form.
first they do m' size 2 FFT's. 
then we'll need to apply eta composition. 

i just don't understand the twiddle matrix.
it's purpose is to transform the output of the p size transform, the input for the m' size transform.
suppose p-size transform is p-tuples of evals at the p'th ROUs. now we want to take p columns, 

so the only reason we got into inverse CRT is to check our answer.
prob better way is long reduction.

ok, lets go on gpu. lets' just work with mod 2^32 first and random ROUs.



we need a clear goal.
lets first confirm what we can do.
i recall roughly we encode commands.
we have pipelines that consist of shaders basically and threadgroup details.
we use an encode to encode commands into a command buffer. we then dispatch the buffer.
each command can be like a blit or compute command.
in blit commands we copy between resources.
in compute commands we invoke a compute pipeline.
in either command we need to set the resources, that is like buffers and textures.
those we can create. 


choices we made:
only use compute shaders, not render or mesh shaders. 
i think we decided against all dynamic libraries.
no function tables.

when should we use a for loop, versus doing all indexing with threads?
suppose there's much more to index than there are threads.
either we use loops in each thread, or i guess it would take multiple pipelines.


can we encapsulate ring elements in a class?
to do so means to restrict it from any thread ops, all must occur in a single thread. 
when it comes to our ring, this may be an option. 
other option is to do it across threads.


maybe they encode pipelines/shaders as modular functions, whereas' I'm programming them as programs. 
in their case, the programming really happens at the encoding level. i wonder about the cost of the context switches. 
maybe we could actually begin not by trying on our own, but by implementing what others have done. 
they often take care of lengths in encoding, leaving the thread just to use its id to perform an operation not aware of global length.


what's the boundary between gpu and cpu? can we think about just what goes on on gpu isolated? think about what resources are in device memory, and how they go to threadgroup memory and get operated on. 
the boundary is commands, those are what we track. 

after we get comfortable with shaders, we'll first have to review libraries and archives and ifnish that. then move on to other metal api stuff. 


using CRT for poly ids:
suppose you want to see f*g+h = 0.
suppose it splits in two. then you are able to test the same equation there. 
now you have two instances of half the size. 
suppose you stop here. then you can batch them together, meaning you compute and commit to the batched quotient.
from the verifier's perspective, the handle to the whole of each part remains a handle to the factors.
randomize the factors, prover commits to quotient, we open.
to open verifier needs to compute eval of the factors, and now multily them together.
seems we have the sumcheck and poly div generalized.
lets indeed try to generalize and formalize.
just think of f*g = h
you always start with arrays f,g of univariate polys. you choose a random input, then reduce to compute the same thing but on the evals.




i like the idea of going through the comp first, holding only the lattice data and the trace, committing to all, then going through again, forgetting about lattice data, and focusing on arithmetization. 




TODO WHEN NEEDED:
fast resource loading (mostly for lookups)
indirect command encoding (mostly for reuse)



i didn't think about how prime is u32, but we're committing in i32.
could we do arithmetic in i32? thats probably what we need. this may change things.
or what if we commit integers using unsigned? so -1 becomes p-1. that would be more complex.

dynamically selecting prime or something. not having any algos fixed, i forget why.


should we do FFTs in parallel?
a round means multiplying
let's actually try to get the architecture right here. 
lets look how others did it. 
but what am I thinking?
we don't want to do it with multiple dispatches, because we want to keep the whole thing in a single threadgroup. our FFTs are small enough for this.
so it certainly must be done with a kingle kernel in a single block.
now in a block, we have many 

an FFT just like sumcheck requires log depth aggregation.
with FFT, you do as many layers as you can inside 



we only know two ways:
1. a new kernel takes care of every round
    poly is in memory. get challenge, each block reads a portion plugging in challenge and write back to mem, but also on reading calcs new poly and aggregates.
2. only do sumcheck inside a threadgroup. issue here is we don't have much memory. one solution is regenerating the data. another issue is many more challenges. 

arithmetization only looks intuitive from the CPU side. maybe we should do it there, and all the irregularity is't a problem. maybe port intermediate results to gpu where commits are made and witness reduction occurs. 


we can't hold that much in shared memory anyway. maybe a solution is to use mutiple kernels, but each reads partial results, recomputes the rest, and writes partial results. 
note how computing the polys the first time (with FFT) is more expensive than the single-evaluation recomputation.



we'll want to use the cpu and gpu both.
smartest to pass from CPU to GPU is probably partial traces. maybe in filtered, sorted orders. 

in general we can say, if there are B units for threadgroups, we can process B times more elements 1/B times the size. biggest issue here is the B times more challenges. we could combine them globally with atomics if we're using a sponge, 


how to do FFT?
so he says can just go to the next round without returning to memory.
the only diff I see is sumcheck must be aggregated. yes, you must return to memory 
couldn't we do this with sumcheck? 

with an FFT, why do you need residency? 

for prefix sum you don't need all block in memory at once. each can calculate it' s result and write to memory. 

if we're going to recompute, might as well do it across blocks. 
i think it's the aggregation that's most costly across blocks. even if don't save all the terms, you have to add them up globally. 

device mem can only hold a few 32 bit words per thread, less than total register files. 

how to make use of execution width?
one way iis using a small field and each handles a copy. but the trace is duplicated. 
so if global memory holds the whole thing to start, 

what's the same position? maybe same 1d index.
maybe it persists for the same pass, which may have multiple kernels. 
how do we set threadgroup memory?

does this achieve the same as resident blocks?
the point was we want a resident blocks. issue was we don't know how many there will be or that they are there at the same time. now I guess it work in the form of threadgroup locations.

if on the GPU, here's what I see:
we basically use each threagroup separately. each one can start at a state and across the group all threads calculate the same trace, but they're all with different randomness. so here it's easy to use a small field. but we don't even need this much parallelism. 
or all threads could be doing global memory reads via imageblocks. 
or we partition the trace by instructions, then a threadgroup reads for an instruction,...
issue is we can't have a block hold much in memory. and if it generates instead it can only use one thread at a time. so if it can only a few kilobytes, do we want a whole proof for it? probably not.
maybe could read over just like memory reads, rest could be generated since it's algebraic. 


if on CPU, here's what I see:
we delegate all crypto at least, to the GPU.
we generate traces and I guess ship the whole thing to the GPU for commit, cuz if we only ship intermediate states it can only delegate one thread to computing it, and also it doesn't know about RAM.
arithmetization happens on the CPU, including all eval reduction, it's just when we finally do the witness reduction that we send over the challenge and it adds. then it can renormalize and commit again.
to keep the GPU busy here one part of CPU is going ahead calculating and sending over data for commit, while another part is behind doing the arithmetization from previous commits. 
and we also pass over a bunch of hash inputs from arithmetization, and the GPU will fully prove these on its own. 

before we g one way or another on arithmetization, CPU or GPU, we'll implement commits on GPU since do those either way.
is commit global? each block reads several elements and commits to them in tree order. 
another possibility is we stream the trace over the gpu for commits, and it get's discarded as it commits, we only need the final output. we'd stream the witness in many parts, basically everything that will be committed.

but remember our logic is algebraic, so must groups diverge?
if not, then they can all start at the top of a function and generate traces. 

First let's just have a big buffer with elements, and each block processes several elements, each group processing one. or maybe each thread processing one. we'll then need to figure out decomposition. output of every block should be a root element.
we'll read the elements to commit in blocks. we commit a block waiting for another. we also iteratively decomposte and commit. 
should a thread or a group commit an element? maybe each thread in a group does it's own, reading an image block. 
this does indeed seem a good starting point, getting it the whole way including streaming over large data sets. 

we maybe need not be worried about lattice size, can be cheap to add more nosie. 

so what's the algo?
first we'll want to read the data. lets use an imageblock.
how to do this?

https://docs.nvidia.com/gameworks/content/developertools/desktop/analysis/report/cudaexperiments/kernellevel/achievedoccupancy.htm

First challenge:
How to read into threadgroup memory? First without imageblocks. 
We can just declare a [[threadgroup(i)]] and write to it, but I'm not sure how large it is. Maybe it points to the beginning of all threadgroup memory. 
we've got our first shader, an inefficient reduction. and we want to profile it. 
i forgot, lets see now. 


recall the goal is to go from non-uniform to uniform.
suppose all the sumchecks were made small. 
actually only the logic is non-uniform and that persists.





We want to implement commits on both sides.
we already did a basic reduction on both sides.

today we implement our commit kernel.
we confirm our algo.

regardless the details each group will commit to one or more elements, and each block will join them together.
we will use imageblocks to read from global. 
but this is an optimization we'd like to benchmark.
maybe first we need to benchmark. 
so we begin with timing counters.









what is the output we expect to compute?
I think it's fully reduced log(S) depth.
yeah, with output that's how it should be.
so we'll also iterate depth log(S)
for each we just apply it. 

so how to split?
well, first we just split in half cuz S=2

suppose S = 4
we're computing index 0 wrong. 
k = 1, i = 0, s = 0
looks like we're computing the high index wrong.
we have
(2 * i + 1) * (S / (1 << (k + 1))) + s
= (2 * 0 + 1) * (4 / (1 << (1 + 1))) + 0
= 4 / (1 << (1 + 1)) = 4 / 4 = 1
this is right as an index of vals.

vals[s] = input[s * (D / S) + t]
vals[0] = input[0+0] = input[0]

Correct: input[D/2] = input[64]
Ours: vals[1] = input[1 * (D / S) + 0] = input[D/4]

oh, we forgot to extend the correct verision.

so what's the naive algo?
we recursive i
at level k, how many decomps do we apply?
at k=0, we apply 1, at k=1, we apply 2, so 2^k
for each we iterate and each is of size D/2^{k+1}
first is of size D/2
so we iterate through this area, offsetingg by this much

k goes from 0 to log(S)-1 (0,1,2)
j goes from 0 to 2^k (1,2,4), counting how many decomps we must apply
each decomp is of whole size D/2^k
l goes from 0 to (D/2^k)/2, counting the indices to fold on
but they need to be adjusted by j. so we need to bump up by j*(D/2^k)

try for k=1
j=0: l from 0 to 64/(1<<(2)) = 16
    hi = j * (D / (1 << k)) + D / (1 << (k + 1)) + l
        = D / (1 << (2)) + l = 64 / 4 = 16 + l
    lo = j * (D / (1 << k)) + l
        = l
j=1, l from 0 to 16
    hi = j * (D / (1 << k)) + D / (1 << (k + 1)) + l
        = D/2 + D/4 + l = 48 + l
    lo = D/2 + l = 32 + l

again, we have to be careful about not overdelegating.
I think we got the algorithm correct. now we need to work it across multiple blocks
for now each block computes for one.
now ever block needs to read into the input at different offsets. 

when G = 2^3=8 we're failing. 
but it works with G = 2^2=4.
fails at 57. seems arbitrary.
how to problem solve this?
can we find any hint where 57 is coming from? its on the 58'th element that it fails, index 57.

we have 8 blocks. in each we have 32 threads. each thread reads 2 values.
so each block should have input 64.
we're failing at an index in just the first block.
it occurs when we enumerate g = 3,4
neither by itself gives the error
oh, blocks are not of size G, but of size D

now what if we make thread size more than warp size?
it shouldn't matter cuz we're not using warps now.

what determines register size? S*T

it can support S = 2^10, T = 2^9, D = 2^19
while it can't support S = 2^5, T = 2^10, D = 2^15
Why?
There are T threads. So the larger the number of threads, the more warps.
Each thread reads S inputs, so the larger S the larger the register space for each.
To support the max threads per threadgroup 2^10, we need S <= 2^4.




















Here we'll consider the performance of just reading and writing. 

SD = 1, TD = 10
    GD = 19: 68
    GD = 18: 31
    GD = 17: 16
    GD = 16: 7
    GD = 15: 4
    GD = 14: 2
SD = 2, TD = 10
    GD = 18: 65
    GD = 17: 32
    GD = 16: 15
    GD = 15: 8
    GD = 14: 4
    GD = 13: 2
SD = 3, TD = 10
    GD = 17: 76
    GD = 16: 32
    GD = 15: 16
    GD = 14: 8
    GD = 13: 4
SD = 4, TD = 10
    GD = 16: 71
    GD = 15: 32
    GD = 14: 15
    GD = 13: 8
SD = 5, TD = 9
    GD = 16: 80
    GD = 15: 33
    GD = 14: 16
    GD = 13: 8
SD = 6, TD = 8
    GD = 16: 66
    GD = 15: 33
    GD = 14: 16
    GD = 13: 8
SD = 7, TD = 7
    GD = 16: 104
    GD = 15: 56
    GD = 14: 26
    GD = 13: 11
SD = 8, TD = 7
    GD = 15: 150
    GD = 14: 70
    GD = 13: 36

What's the pattern?
The theoretical pattern seems to be
27 -> 8
28 -> 16
29 -> 32
30 -> 64
We have not preference on size if this were to hold.

The last two, however, take double time.
27 -> 16
28 -> 32
29 -> 64
30 -> 128

The best we've got is D = 2^30 for time around 64, winner is SD = 2, TD = 10, GD = 18, time 65
But we see that as we increase SD and decrease TD we get worse for large sizes.
But we want larger SD to have less warps to communicate across. 
It's not too bad. so i think we don't have to worry about memory reads. this is the cost of memory. now we see the cost of arithmetic. 

Now we try these same params but with arithmetic.

SD = 1, TD = 10
    GD = 19: 
    GD = 18: 
    GD = 17: 
    GD = 16: 
    GD = 15: 
    GD = 14: 
SD = 2, TD = 10
    GD = 18: 65 / 61
    GD = 17: 32 / 30
    GD = 16: 17 / 15
    GD = 15: 10 / 7
    GD = 14: 4 / 4
    GD = 13: 2 / 2
SD = 3, TD = 10
    GD = 17: 
    GD = 16: 
    GD = 15: 
    GD = 14: 
    GD = 13: 
SD = 4, TD = 10
    GD = 16: 238 | 60
    GD = 15: 125 | 28
    GD = 14: 66 | 14
    GD = 13: 43 | 7
SD = 5, TD = 9 ????
    GD = 16: 
    GD = 15: 288
    GD = 14: 159
    GD = 13: 83
SD = 6, TD = 8
    GD = 16: 280 |
    GD = 15: 142 | 
    GD = 14: 78 |
    GD = 13: 42 |
SD = 7, TD = 7
    GD = 16: 350
    GD = 15: 175
    GD = 14: 87
    GD = 13: 46
SD = 8, TD = 7
    GD = 15: 651
    GD = 14: 364
    GD = 13: 177

There the last two are no longer approximately the samee, but now the last about twice as slow.









another choice for primes would be cyclos 31 and 16, both requiring 2 challenges. both mod 2^11. both primes only 2 terms. 


right now we got:
rolled, SD=3,TD=5,UD=2,WD=4,BD=16, time~500 (this is great... if we can keep it)
unrolled, same, time~64. with primes and WD=3, time~2.4 seconds.

note it would probably be a good idea performance wise for a kernel to process more inputs than it can hold at once. storing the results in shared memory, accumulating them as we go. 


i'd really like to do karatsuba.


complex fft:
can floats recover 64 bit integers? of course not. when you invert you'll get 32 bit values.
so it seems you can work in another field, but you'll not only need to invert, but you'll need twice the precision. probably not worth it for us in any scenario.
so we're then left with Karatsbua or Schoolbook, anything else? 

maybe we do schoolbook and generate bits while simultaneously multiplying by powers of 2. 
one benefit of schoolbook is we only partially reduce each and accumulate in 64 bits without reduction til end. 

as for offsets, can we not additivel handle it?
for a single constant, consider h*c where h is how much to shift. you want to subtract this from the commit. you do this for all of points that have been aggregated, which turns out to be the sum of all constants in that column multiplied by h. so indeed as long as we accumulate the sum of all constants we can do it this way. 
actually that algo requires twice the space too since inputs are read-only. we might as well roll our own.

maybe we can try karatsuba on the top level for it's convenience and then schoolbook below.

(p0+x^4q0)(p1+x^4q1)
p0*p1 + zeta*q0*q1

probably the most problematic is the multiplication of the inside polys. 
maybe we should try the space efficient version. it takes the same space as schoolbook. that is 2n.
actually 

an issue with karatsuba is it will probably require all 8 constants present at once. in that case we might as well make state that large, in which case to save anything we'll need longer chains.
now we want to start with schoolbook.

schoolbook:
keep an aggregation array.
multiply by each term of the constant poly.
that means shifting up by a power, multiplying the lower parts by zeta. then multiplying all by the constant.
we will instead multiply zeta by the constant once (maybe can re-use this in the seed), and then multiply correspondingly in the current state and add to the aggregate.
ohhh, the fact that we go block by block not only lets us do a longer chain, but it means we only need that much. extra space. 
how large is a single element? 
oh, maybe we don't need

i think i remember 32kb, that's 2^15 for the register space (as for shared memory).
we have 2^10 threads, so each thread can only have 2^5 registers. indeed that's what we have, SU=32.



you can start with all 1's in 

maybe worth thinking for a moment about which is superior in proving. 
lets call them comb and reduction.
first note we can prove these pretty easily by just using precomputation to them. the commitment need not be computed by the prover. instead the prover needs to hold the constants in memory and take the random comb. note we don't need to extract, thankfully, so i think we can recursively fold down. we must use smaller constants to do this. so here we could use constants with maximal nodes, so maximal folding factor. then how to prove the final ones? this is an interesting problem. i guess what we can do is use the fact we need not extract, and 
hmm, whenever data is a deterministic expansion derivative of data the prover has knowledge of, we can assume the prover has knowede of that too (assuming prover has knowledge of the algorithm), and therefore we need not extract for a commit to it. so this applies in the case the constants are generated by a seed known to the prover. this seems a good security practice, except that it doesn't allow folding across statements.
we have trouble widdling down after reaching minimum radix. i guess this is where we want to use the generation. so this actually means we only need to generate them in zkp friendly way for min-radix trees and at the end, so we won't need long chains, and most of the time we can use non-friendly generators. now note we'd want to have as few seeds as possible, simply because that's entropy, yet we can still break them up as parallel. 
so i guess most of the time it makes sense to use a good generator and a fast one no zk friendly. 

lets suppose seeds and states are about the same size, maybe 64 bits. 
we're thinking to apply it SU times at least.
lets get started with this. then hope

we know that we can have the id property since we can factor out the corresponding point, and consider that implicit. 



https://arxiv.org/pdf/1811.04035.pdf
https://www.math.auckland.ac.nz/~sgal018/compact-LWE.pdf



how do we streamline the unrolling?
in the build process. 
first issue is that the constants need to be the same across the build script and the running script. 

oh right, we want to read in groups of U. but U may well be beyond vector size. 
if doing u8. should we have U at most 4? remember we actually like large U. well, U in place of what? in place of S 
so suppose U < ord. then we need post arrangement. so i guess we'd really like U=3, so no post arrangement.

what if U > 4?
then we'll have to split into another data type. so you'll have to load eg u16 and split into two u8 values. how to do this? the only way I can think of is shifting or masking. this takes operations. we don't need them now. we write custom for each U. for now we'll always have U <= 4.

we're always operating on the U values in parallel, right? so why not have them as a vector for fewer instructions?
u8
U=2,S=2: 69.7
U=1,S=3: 65.4
U=0,S=4: 74.2
u16 (half the output)
U=2,S=2: 36.9
U=1,S=3: 34.9
U=0,S=4: 34.7
u32
U=2,S=2: 46.9
U=1,S=3: 46.0
U=0,S=4: 46.1

so not much of a difference, but this is just reading, maybe it will change when we have hold all the data. 
lets try multiplication. suppose we've already shuffled the data, so 
here we'll need to access the vectors. 

note how we're gonna have to write to shared memory and read in order to add. some warps will suspend after writing.
one option is to write before multiplication. then a thread can read all terms for a factor and add them together.
otherwise, you'l need to write multiple times to memory to share partial addition results. but an issue here is we dont want to generate along this direction. 
if we multiply first, however, then we can indeed add this way. but this will mean non-sequential access, unless we can have each thread read part of the same element

how do we 
our issue with multiplication is the extra space it takes. up until then we just need the space for the poly. 
this may mean we can't do a full element in a warp. that's fine, as long as we don't have to shuffle between warps. do we? well yeah but not until the bottom where irreducibles are hend between warps. here we could multiply the outer parts for karatsuba

how do we interact between rounds?
remember we're assuming that we add everything up in a block even if this means a lot of seeds. 
so after we finish with an element, whether or multiplied or not, we write to shared memory which should be able to hold all of them since shared memory is about the size of the register file. there's no synchronization here. instead each warp continues at its own pace to reading and writing the next element.
Every time it's done now with an element, it adds to the one in shared memory. then all thread sync at the end.
and here is where we drop maybe even all the warps except one that handles adding the 32 elements in shared memory, then writing the sum to global mem. 
the multiplication can has to be done 
biggest question still remains of when and where to do constant generation. we're not doing any duplication in a threadblock. so we make the chains as long as possible, 
uh oh, we'd like to generate seuqnetially across the invocations of a warp, but these are added together. everything in the warp is eventually added together. we'll have a multiple of 32 elements added. I guess we can afford an independent seed for each of them. but then the multiplication within a thread becomes sequential.

can we share the space for generating the constants and multiplying?

i didn't realize, we need to generate a constant term for every factor.
wow, does every factor need it's own seed? 

suppose we add together 128 of them. that's 4 rounds.
then we can have identity for 1 factor 

we were thinking to go sequentially across the blocks held by a thread. that's SU. this is fine. 
oh, it's also fine to go across the invocations if they don't get added together. 
but then we'll have to rotate what part of an element, each 

maybe it's worth generalizing to where we don't have the special case that an element fits inside a warp. in this case we can't do multiplication. but we will at least go until all reduction arithmetic has happened. then there's just rearrangement to be done. this can be done by writing to shared memory in a certain way, hopefully.
then you could read from there and multiply, then write new element by addition. 
but this would be like that hermite form.
so how do we multiply when an element is spread across threads. 
even if an element is in a single warp, we probably don't have enough space to multiply. well lets sdd. 



maybe worth thinking of how to best layout generators. well for the binary generators it doesn't matter.
but for zk friendly, to make tree reduction fiendly, maybe have a seed for each tree layer, that operates in parallel the outputs of the parent. 


so indeed, we're right about the double space being a problem. even halving warps doesn't work, I guess cuz it's its too many registers for a thread.
so this is a definitive answer that we cannot fully reduce and multiply an element of size 1024 in a warp. 

each thread can use at most 255 registers. what are we using? 2^3*2^2*2 = 64. so that can't be the problem. and we only have 1 warp. we're failing even with a single thread. 
looks like we had some false info by not initalizing the array (somehow it would give 0).
actually the aggregate doesn't seem to make a difference. 

if we're going to multiply the blocks in sequence, then we can write each one to memory as we do so. each thread could have it's first block identity, so just write it to memory as is. then use that space for the generator. go to the second block etc.

lets just try schoolbook correctly but not concerned with space for a moment.
so what's the number of blocks? 

maybe we should complete data arrangement first.
how so?
how to do data rearrangement?
we have data in two forms.
on the gpu we write into arrays, modify and even exchange among arrays, and then write back with the same mapping. we'd like to be able to view the same state on cpu. or maybe at this point we only want to test the final rearrangement. what is that? it's the coefficient arrangement.


suppose we decompose down to irreducibles.
once finished the irreducibles are still spread out among threads. how so? every thread holds S block parts, each block is size, well it's huge unless we've decomposed. if our ring is size 512, then actually it's size 4. lets go down to fully splitting. that would be size 128. 
we're at size 256, order 2. we decompose the full way. each thread has S=8 parts. we still need a single swap. 
we're calculating in two ways and comparing. one is by reduction. we'll want to reduce down all the way except the last. while there are 256 total, we'll only want to go til having 128 factors. second, we imitate the gpu, in thread form. 
for first agreement, let's do the partial reduction. so we'll reduce to 128 elements. and we compare that with our thread reduction.
ok, we got that. now we need to apply the final swap. 
it seems here each thread contains offsets by 32, and 8 of them. 
does that make sense? there's 256 bits total. there are 32 threads.
32*8 = 2^5*2^3 = 2^8 = 256. so this is correct. 
now there's 128 blocks. we'd expect them to be in 32 groups of 8. 
so actually they're not in groups, but just offset by 32. is this what we expect? what do we expect? well this layout is indeed what we expect in relation to the original coef form. but here we're comparing with the reduced coef form. well i guess that's our target.
we should reduce from 256->128->64->32->16->8->4->2, that's 6 times. each thread here as 8 values, so can only reduce 3 times. we'll decompose the rest using cross thread logic. 
and we end up 

whereare we at the end of independent thread reduction?
we've done three layers of reduction. so there should be 8 factors. each thread holds a part of each factor.
lets try the first round of cross-thread reduction. we should split the threads in two. each block should now be cut in half. where is block 0? how large is block 0? it has 32 elements. those 32 elements should be dispersed across the bottom 4 threads. 
every 4 blocks should contain consecutive elements. so the offsets between elements in a thread should still be 32. and between threads they should still be consecutive. the 5'th element should not belong to index 0 of thread 5, but rather the second element of thread 0. 
we decomposed across threads once and the layout is the same. every thread still contains one from each block. 

block 0 is had by the first index of all 32 threads. 
we want to split this block in half. 
so thread 0 should exchange at index 0 with thread 16. 

thread 0
2240599212
thread 16
1331822491

we don't want to look for the relevant zeta, but we know that summing them and dividing by 2 should yield the lower.
24789579 + 178153484
and indeed it does.
at this point we should have split the threads in 2. 
now thread 0 shouldn't represent all blocks, should it?
thread 0 exchanged with thread 16.
i guess thread 0 should represent every other block.
well actually thread 0 seems to still represent all offsets of 32.
when a block splits, where do the two parts go? consecutive, right?

so what's the current layout after one round of cross-thread reduction?

what's different?
before thread 0 at 4th element would represent halfway through. 

before we had index 0 across all threads representing block 0.
now block 0 has split in two, but still occupies the same space. 
at the same time, the zero indices of all thread have interacted, so the two blocks still remain represented in that index. 
hmm, actually seems that yes, there's no difference in layout except algebraically which you can't see. 
oh wow, so at the end while a thread only represents S blocks, they are separated by 32. i thought we were getting closer, more locality. yes we are. a block is now represented by fewer threads and they are continuous. well actually that's always the case, now they're just smaller.
so we should be able to look across threads to get the blocks. again this is just algebraically visible.
so if we have blocks size 2, block 0 should be had by index 0 of threads 0 and 1. we'd also expect block 0 to appear as at the 0,1 indices of the regular reduction. isn't that the mapping that's always true? 

so does the layout change or not?
we know the layout of a thread doesn't, and therefore nothing should change.

what would happen if we continued to reduce beyond irreducibles, but just exchanging data?
we thought before that we'd end up coalescing blocks inside threads. 
but in order to do so, we need to reduce the number of blocks a thread represents. 
suppose we wrote to shared memory as is. then it would be contiguous. if we instead exhanged data we'd be writing to every 8'th element. would this cause a bank conflict. we can assume 32 banks, so this would cause a 4-way conflict. one way around this is to offset each write by its thread index.
to count down the elements in coef order, count down in threa major order. 

so maybe we need a different kind of data movement. 
do we really want to do this data movement?
or do we want to try multiplying while they're distributed?
issue is we don't have the constants. well actually the constants could stay put, and if we're doing schoolbook, we just swap around the commit data. 
how about using Karatsuba? 

simplest might just be to write the decomp to shared memory, then start generating the 

lets do karatsuba. it saves time. we can write half the data to memory, then use that half to multiply the other half. 
major question is where we use chains. we long longer ones simply in order to have less to load. if we were to load them, we'd load them in parallel but with sequential instructions, so maybe it's just as good to generate them in sequence. do we use them across blocks? yeah i guess so. that's a 1024 factor. and i think it would be also fine across invocations.
so how do we iterate through the blocks and generate. 

maybe in order to max occupancy we don't want all elements in a single warp. 
but they have to communicate at the start of cross-thread, right? no, we could wait to have them communicate at the end? that is, could we have threads first communicate 

what if we had half the rank?
we could tolerate this, it would be rank 512 with degree 4. 
what's the cost? well it means twice as many constants. but here generating them it may not matter.
there's less for challenge, more like 18 instead of 16 bits. which is fine. we still do the same number of NTT layers. now for each element there's 2 multiplications degree 4. in total 4 mults degree 4 rather than 1 degree 8. how does that compare asymptotically? 8*3 vs 4*4*2, so 24 vs 32.
what's the benefit here for us?
it's the independence. we can more easily fit an element in a warp, fully reduce it, and sequentially go through all multiplications needed. each thread only needs 16 registers for reduction. then we only use a little more in order to sequentially generate the constants and also 

does this 512 ring help with other cyclotomics? p=31, with order 32, works. that would be interesting. how to reduce once we've filled up a short? 2^5=1 => 2^{k*5}=1 so you take chuncks of 5 bits and add them together.
these extra 5 bits may be all we need. 
note it would require another NTT but with the same input, so done by the same warps... in sequence, so we could continue the chain. 
here it's the NTT that's probably not worth the few extra bits. that NTT requires the same amount of time, but I guess the good news is it takes less storage and the zetas are small.
also cyclo 2, p=3, so you take all odd bits and add and all even bits and subtract.

another is cyclot 31, that's 2^31-1, but here the issue is only 60 bits.

i imagine we'd try to compute the NTT's in parallel cuz sequentially we'd need to keep the input. 

oh damn, maybe we need modulus more than input, so we can only encode trits using p=3, and only encode 4 using p=31.
no that's not true. we can think in terms of CRT composite. it's just the absolute number that matters.
most convenient choice for now seems to be H or p=2^31-1 (cyclo 31), along with p=31 (cyclo 5) when we need it. 
while cyclo 31 doesn't have power of 2 roots, it's arithmetic is faster. but oh, shit, it doesn't have any roots, but that's fine, it still splits. so for now, with this strategy, we can easily swap the prime H vs H' (2^31-1), but we need to develop NTT for p=31 alongside. 
with p=31 we're thinking for 512 ring. 
oh fuck, isue with 2^31-1 is two challenges. this may not be that bad. if we go for 2 challenges. 

note we could also use both H and H' and ring size 512, and only 1 challenge? no. using H' it still splits. 

generating two challenges won't be too hard. computing both also won't thought it takes twice the space. it's the renormalization of both, and commititng to both that's most costly. but we'll see, cuz H' arithmetic may be more than twice as fast as H. 

so our question now is whether or not we can do better with 512. the benefit is holding less in memory. you only do degree 4 mults, and two of them but you can in sequence, and there's twice as many elements to commit. but doing the constants in sequence (writing them to memory as we go), we can say with 512 it takes half the memory. so would we like two of half the memory instead of 1? yes i think we would. maybe we try both. maybe best of all is the simplicity.

of course we want to specialize right now but with performance around the best, so not sacrificing.
now first thing we can try, is having U=4, with bytes and u16's as inputs I guess (compare), and in this case we shouldn't have to do any post-decomp movement. 
suppose we try this. we need to multiply dgree 4, probably with Karatsuba.
note it requires the same number of zetas, and in fact the same ones. 
a0+a1x+a2x^2+a3x^3  b0+b1x+b2x^2+b3x^3
so we'll multiply these within single threads.
upon multiplying the top two, they'll get added to the bottom multiplication, degree 2.
then the middle is also degree one, going to quadratic, but then lifted up to cubic.
do we perform these in parallel? maybe we generate all 4 constants at once, then we multiply in parallel.
or we generate and multiply at the same time. the issue here is we basically need all the constants at once, right? you need to multiply bottom, top, then add. so yes. 
we need space 12. naively how to implement?
how much do you need for linear?
you have space 4, though output is space 3.
(a0+x*b0)(a1+x*b1)
0: a0*a1
1: a0+b0 -> 1*3-0-2
2: b0*b1
3: a1+b1
so this means we can take the bottom and top and multiply in place.


(a0+x*b0)(a1+x*b1)+(c0+x*d0)(c1+x*d1)
0: a0*a1+c0*c1
1: a0+b0 -> (a0+b0)(a1+b1)
2: b0*b1+d0*d1
3: a1+b1, c0+d0
forget the linear mults. can we do the 

a0+a1x+a2x^2+a3x^3  b0+b1x+b2x^2+b3x^3

i'm now wondering if schoolbook also requires 3n. a native algo does. only way not to is to generate the constants one or a couple at a time. at size 4 we probably won't do this, at least not to a big difference. 
so maybe we just use 3n and do karatsuba. 

would 1024 seeds be too much? probably not. but each would only be length like 4*4*64 = 2^10, or whatever such that starting with 1024 seeds, we have a thread group 

actually, a reaonsable solution so make chains 32 times larger would be to share among threads in a warp, starting the generation at the beginning, swapping among threads as we compute ntt. this would be good use of space. i guess this means generating all the constants for a thread before passing it to another... but this is duplicated code...
maybe better to still start with 1024 seeds, but instead of waiting til end to generate we can do so during ntt. then we'll need an extra n space. 
alternative is one warp generates and writes to shared memory. it would have 32 seeds, each can be responsible for a simd lane index. it would probably be faster than the NTTs. ok, this is nice because we can take advantage of that read-only mem, so we only need one extra n space rather than 2. note we'll have to synchronize. threads can overwrite the constants as they give output, but we'll need enough shared mem for both the constants and also accumulator... there may not be enough. 

interesting. for 6,6 it says not enough registers. for 6,5 or 5,6 it gives 0, and for 5,5, it copies as it should. so even here there's the issue of 0 output. why? 
so hopefully reading and performing comps take the same mem. we can with 16 warps get up to S=2^6,U=2^5.
with 32 warps we can get up to S=2^7,U=2^4. that's a lot more than we need. maybe lets go ahead and try karatsuba for 1024 despite the extra space. 
now i'm back a couple days later and with 32 warps just io with U=2^4 we can't even get S=2^1. so maybe it is indeed behavioral, drastically. that's a factor of 2^6.
but before, that's 2^11 registers per thread.. a ton, 2048, could that really be?
actually before we got similar behavior 2, 32 registers per thread, 512 threads.
can i complain about this alone? I though register file is 32kb, that's 2^15.
we have 2^9 threads, and 2^5 registers in each. so it makes sense. hmm. this is in rolled form. lets try unrolled. i'm getting about the same unrolled, but we can get 2^10 threads. 

what if we check poly instances with random combs. you'll have to compute error terms, but maybe not quotients. 

are we going to want to do arithmetic in vector form? would this allow for optmizations? here the issue is we seem to need more space. 

now we can't even get SD and UD to add up to 4. before we could. maybe it's this business of processing in groups of U.
yeah, doing U serially seems to save space, 1 bit of space. 


lets move on to doing multiple blocks.
we'll have a loop that processes elements then adds them up in aggregate. 
we need to figure out where each round reads from. 
each round can read from a different part of global memory. remember we want to be able to write back to the same area at the end, and we want them to be continous. 
so where do we read from? suppose each read continuously. then on writing back results would be scattered. we can have blocks read sequentially. every warp should read it's first as it is now, then it should read the rest at appropriate offset. whats that offset? it's the amount read by all warps in a block. each warp reads one element, so for W warps thats W*D. 
within a block then, a thread has an offset in this range, and in the q'th round it reads at the same index with offset q*W*D



the following works \% P
pub const ORD: usize = 8;
pub const SD: usize = 2;
pub const TD: usize = 5;
pub const UD: usize = 3;
pub const VD: usize = 8;
pub const WD: usize = 4;
pub const BD: usize = 8;
that's a total 8+4+8+3+5+2=30
it's curious how S uses more registers than U. either way we can fit D=1024 in a warp. 
seems S=2, and U=3 is the best we can do. U=4 or S=3 both return 0.
but thank god for this, because U=8 is what we need for D=1024.
following also works.
pub const ORD: usize = 8;
pub const SD: usize = 2;
pub const TD: usize = 5;
pub const UD: usize = 3;
pub const VD: usize = 9;
pub const WD: usize = 4;
pub const BD: usize = 7;
with \% P takes 25 seconds, with u32 takes 300. note still rolled. 
surprisingly writing at only the end rather than at every iteration doesn't save much. 
also surprisingly (see https://www.desmos.com/calculator/zugscplhmz) larger V doesn't make much of a difference for performance, so the difference for us is just longer chains benefit. we still might get best performance with just a few blocks like 4 or 8.
but we should only seriously consider these tradeoffs when trying with unrolled and our number system. 
wow, looks like by 2^30 we're committing to exactly 1 gigabyte, great. maybe by ridding of output we can get 2GB.


what next?
we still need to unroll and also put in number system. but those are independent.
let's first complete kernel.
we aggregate in our acc. at this point we need to do the final reduction.
this means writing to shared memory. (note we should later try aggregating in shared mem and see if we can increment SD or WD).
so once each warp has aggregated, we write i guess just directly to shared mem. 
each warp has an element size D, there are W warps, so we'll need DW space. 

or what if we just do aggregation right here using atomics? oh, we could use addition in 64 bit. then we'd need one thread to reduce them and write result to global mem. 


i still don't see how to set threadgroup shared mem. we don't want to declare fresh inside each kernel. so instead we pass in a pointer. i guess this pointer points to the beginning of what we allocated. 



lets try to send 4-vectors with subgroups. but not if we have to sacrifice registers, or maybe, just check speed. 

i realized we probably want to do multiple commit passes for one commit. because we can have huge radix, like input 2^30 and output only as many second layer nodes as blocks, which is prob less than sqrt, but still large enough we don't want cpu to do it. so we'll send over more input size n-sqrt{n}. so we have the ability here to pile up lots of commits. can we use this? just think of sending over tons of input for a single commit. yes, that was the plan, we do the execution trace and commit to it, and then go through again. what if we're not aggregating all of them at once? then we'll end up storing blocks of it to be normalized, and note we can send these over for commit since the gpu was done prior with commits. there shouldn't be much of a gap of cpu waiting for commit cuz we send it less and less until the very end. 
would be great if we could reach a terrabyte in a subsecond.
now what's the structure for folding? it's consecutive subtrees. 
they don't all need to be the same shape. and i think we can have the input segments folded and the input segments sent completely independent. folding segments are determined by arithmetization, we don't care about it on cpu. but we still need to count somehow. 
the question i'm looking at is how to combine the intermediate nodes with the new input. 
one possibility is the natural way for the gpu where we have a starecase. in this case you go through and fold the top stair, then next, and you'll end up with every step having larger norm, and more steps, proportional to many blocks you committed to. 
eventually when we'll have to start reducing the intermediates. maybe we can dedicate a block/warp to processing the latest intermediate nodes, but probably not. so we don't touch the intermediates of next layer until we've writting all those of the current.
now how do we structure the tree? a different kernel will handle each tree layer, so we can have different radixes (also different constants or rings but less likely).
actually maybe easiest to begin is layer 1 tree, where we use new constants, those are new (truly random seeds) encoded in the new kernels. so we can go for as many kernels as we have. 
if we do this, we every round just write input to same place then launch a new kernel. we'll get the best performance with this, and it's simplest, so we begin with this. oh wait, you don't read decompose those but rather add to them. so you decompose the rest then add to them. so hopefully it's the same kernel, just different seeds. 
now we started this to consider where to write outputs. at offsets would be nice, m only concern is we'll need the buffer for new input to be almost sparse but large. we'll have a heap. yeah i don't see a way for offsets. if we don't want sequential then have a different resource for each. but we want to overwrite. so we'll try contiguous. 

right now we're trying to write back appropriately.
we have vars b,v,w,s,t,u
is that the correct order?
an element is std. does w or v come next? 
if v, then each warp reads contigous across v. that doesn't help much.
but it may help to do so across warps, so we make w next.
then what? then probably v. actually probably not. b should be next, then v, so that v=0 are continuous across blocks. 
maybe that's wrong too. we're only writing, not only to v=0, but to w=0. therefore we need
v,w,b,s,t,u
so this means warps read from different areas, but 


says 16B reads are most efficient on M1.



apparently we should use global atomics, fast. wow. if we use 64 bit, we could indeed have a single level tree but it requires all threadgroups to use different constants. 


unexpectedly, wheras reduction with shared memory doesn't take any more registers (though shared memory), we lose another bit. 
frustrating, makes no sense, S=2, with U=3 works, with U=2 splits out zeros, not even using shared mem. 
so the U problem is one thing.
then with U=3, it seems the issue is the barrier. even if not a memory but an execution barrier, it fails, suggesting maybe it can't run all the warps at once. 

so we can ask, we have a kernel, high register usage.
we try without execution barrier and it succeeds, then with execution barrier an it fails. how can this be if all warps are resident at once?
similar question about register usage.

https://developer.apple.com/forums/thread/725908

with this unpredictability i'm realizing we may just need to do a lot less in each threadgroup, and this will require more complexity of reducing cross-thread. 
ok, luckily now it's failing with SD=UD=1



using cyclo 61 may be a good idea. noting we don't have to check intermediate nodes for smallness.
and it's fine for fft cuz it splits into quadratic terms mod any power of 2. 
we take advantage of cpu 64 bit mult. only need 1 lattice challenge. the issue is multiplication on the gpu. of course we use karatsuba. but note this take 3 mults rather than 2. it's the reduction that should be faster than 2 reductions. so split into low and high parts, storing them in u32. multiply bottoms and tops. but how do we split? i guess 31 and 31, so top exponent becomes 62, so 2^62=2^61*2=2 so you multiply the top by 2 and add to the bottom. then you add together bottoms and top and multiply, and if the same shift on both, say 31, then you'll take like the top some bits and they'll go to the bottom. 
note we can prepare the zetas to be shifted instead by 2^30 so we can get exponent 2^61=1.
now we'll still have order 2 terms, so multiplying these is not ideal. but this one is still worth trying. 



what can we say when we evaluate on X^n-z? well actually this can serve for fft, right? i mean, does this give you info on n eval points? the question is whether it determines n points, determines enough for poly product. yeah it does. but you'd have to multiply. we were planning to amortize these, but not actually multiplying, right? no, we need to multiply them, maybe unless you want to commit both and we dont. so this is an issue with ffts. whereas with H, we can do fft small enough so it fully splits, FFT's of size 128. or maybe 256. this should sakve a constant 8 sumcheck rounds. but that's actually just regular fft noting multiplicative group has 2^8. also has 3 and 5 or 7 i think. so maybe just regular fft better. 




trying schoolbook. 
we go through the powers of the constants. we have an aggregate array.
on every shift, we would bring the top to the bottom and multiply by zeta. until you've multiplied by degree n-1. 
you rotate n times, first time not at all. 
each time you rotate you multiply the shifted element by zeta, then all elements by the constant, then add to aggregate. but 

we can basically assume due to randomness that it will overflow. so if we aggregate in 64 bit space, we know it only overflows once. so answer is correct answer mod 2^64 = 2^32*2^32 = epsilon^2. so you can easily reconstruct the result by just adding epsilon^2 or something and reducing. while taking twice the space it saves a lot of reductions. 

now what are the zeta values?
they are the 128 irreducibles. we reduced using these in the last round. they should be the same ones. 
first lets try 512. 
first of all, we should already have it in proper reduced form since it has terms dgree 4 and U=4.

lets try smaller first. lets try with block size 0. then we need size  at least 256. 
what do we expect to see? well we expect the same lexicographic order. it's just that now with U=2 we expect to see it in blocks of 2, and with blocks size 2 weknow they correspond. ok, we see what we should. 
so we're relying on the native reduction to be correct. we expect it to split ou the factors in correct order in correct coef order too. maybe we can check on wlofram alpha. great. it works. 
now we just go through and do aggregation. 

we need to identify the correct zeta for each block. 
first of all, the zeta level should be the last one, that's k+2 = log(S)+logT+2
then we'll pass across i, but actually every thread will only pass over S i values. they're in sequence. so thread in index t skips over the first t*S i values.
lets make sure of the zeta level. we have 128 factors, but actually they come in paris, right? yeah. so this can optimize our multiplication, right?
well no, we're no longer splitting. it only havels the number of zetas to store. so we're going to access the same zeta twice. one is the additive inverse of the other. so we'll just aggregate by subtracting one of them.
remember when we split we add on the bottom and subtract on the top. the true zeta belongs to the bottom. inverse on the top. so we'll subtract on the top. 

we can also just do karatsuba allocating temporary arrays relying on copiler to handle

todo:
generate the code.
change number system.
write output to shared memory.
invoke multiple times, chaining and swapping seeds. 
try storing all constants.
try karatsuba. 
adjust for post-decomp movement.
try with other primes like 31. 

we can add permutations to the generation, as in PCG, later if we decide to share among multiple threadblocks. the two reasons we wouldn' do this is if we don't share among threadblocks, in which case we'll want it zk friendly and it's so short lcg should be enough, and the second reason if sharing between threadblocks means writing to global memory, and seems you're not really saving space there, maybe better to use constant address space. 

we're going in sequence for each of the S blocks. after each block we're writing to output. this may not save any space, but hopefully it speeds up the writing.
for both schoolbook and karatsuba we'll generate the code. for karatsuba, do we generate all constants first? do we allocate another array for them? 
for schoolbook we only allocate an array of S, and the state and seed. that's minimal.
for Karatsuba we need to read multiple of the constants at once. i'm gonna guess we indeed need the generate them all before we can multiple. so here we'll need to allocate an output space size n (analogous to the aggregate), the difference now is we need to allocate for and generate all n constants. then the extra log(n) space we'll just dynamically allocate with registers. 


how much space will karatsuba take?
for degree 4, we need to store the sums in another array. then we multiply three linears in parallel using this space. 
does this generalize? well you'll have to allocate more space but it could be done additively, though not in parallel. i like this. we can dynamically adjust the space vs ILP. I think it's n+n/2+n/4+.... = 2n. that's additive, or its' n+3(n/2 + 3(n/4 + 3(n/8)))... i'm just thinking about like for 31.

starts out with h=0, and f0=0, and f1=f


how to generate this?
we need inputs to be in the form of slices. 
can we do it by passing in plain ids? we may want to pass in A[3..6] and instead we pass in A', so it generates instructions A'[1..2]. then we need to translate this to A[3+1..3+2]
so how to specify indices so we can compose them? closures? if it's a string we'd have to parse it. return is a list of instructions which refer to indices in the aruments. we want to parse these instructions. so an instruction needs to reference ids along with indices. 
of course ids can just be indices too, indices of the arguments. so the only argument is then k. 

ohh, i forgot about the subtraction of the outsides from the inside. 

how might we do it?
hopefully we can avoid the complexity b overwriting the input.
you can start with the outsides. you multiply the two and place appropriately, could also do so summing. then you add and place on the inside (or wherever) overwriting. then you add that to appropriate output, in special case rotating it. 
suppose you do middle first. thing here is you have add them first. where do you put them? you could put them on the outside of the output. then you multiply into the middle. no, you can't do that. so lets just use our simple algo.
oh no, recursion doesn't hold. we can't modify the input when computing the outsides. cuz we'll still need to multiply. 
oh, for them the h is only in the bottom, since they're not folding.
below the top level we're in the same situation. in this case we can write i guess you their algo, cuz we don't want to modify the inputs. but at the top level, we can modify and we also don't have the spare room. does this work out?
after we multiply top and bottom, we can wrie the sums to anywhere in the input, suppose the two insides. so it's an issue. we can only call invocation overwriting the top output. and if we use their algo at the top we need twice the space. 

so can we manage for this karatsuba?
we can multiply the parts, but it seems only if we use an extra n space.
lets try that. we'll generate the constants. we'll have an acc same length as array. then we'll have an aux same size as a block, that's U.

what about the single vs multiple f? first time i guess we launch with f0=0. can we reduce instructions?


A = 0,0
B = 2,3
C = 2,3
D = 2,3,0,0

h0 = 2
h1 = 3
gamma0 = 25
gamma1 = 0
alpha_0 = 4,
alpha_1 = 0
beta0 = 9
beta1 = 0

0: 2,3,_,0,0
1: 2,5,_,0,0
2: 2,5,_,5,0
3: 2,30,_,5,0
4: 2,30,_,30,0
5: 6,0,_,30,0
6: 6,0,_,30,0
7: 6,24,_,30,0
8: 6,24,_,9,0
9: 6,15,_,9,0

we managed to generate code, but the algo gives the wrong answer. to debug. 
now lets generate the schoolbook code. 














