

now for CRT
first on CPU

lets try cyclo n = 1024 with H
lets first figure out the ROUs

2^80 = 1
2^40 = -1, order 2
sqrt{-1} = 2^20, order 4
2^10, order 8
2^5, order 16


we want to use all of them.
there's 80 of them, the first 80 powers of 2.
which ones do we need?
n = 1024
ord_n(p) = 4
m = n/ord_n(p) = 2^10/2^2 = 2^8 = 256

we need the 128 primitive 256'th roots of unity.

max divisible by 2 is 80/40/20/10/5

so how do we find the 256'th ROUs?

we look at the multiplicative group.
2^8*...
subgroup 2^8 is these 256 ROUs
now what are the elements?
we just need to find 1 generator.
we'd like 2, but that generates group order 80, which doesn't divide 256
oh no, 2^5 generates order 16, which divides 256. but it's not primitive. 
so maybe these 2-powers will only serve for arithmetization.
here we need elements of order 256.
so how do we find them?

since n is a power of 2, we can split in 2 each round.
m=2
    1 monic factor of degree n/2
    0: 2^40
m=4
    2 monic factors of degrees n/4
    0: 2^20
    1: 2^60
m=8
    4 monic factors of degrees n/8,
    0: 2^10
    1: 2^30
    2: 2^50
    3: 2^70
m=16
    8 monic factors of degrees n/16
    0: 2^5
    1: 2^15
    2: 2^25
    3: 2^35
    4: 2^45
    5: 2^55
    6: 2^65
    7: 2^75

at each step we take the poly and split in half, cuz we have X^{m/2} = zeta
we're in power form, we split in half, multily top half by something different and add.
what are the cosets?
all of them will be 

start with X^{n/2}+1
split into factors
X^{n/4}-2^20, X^{n/4}-2^60
we multiply tops by 2^20, then add and subtract from bottom.

split each into two factors
one is now 2^50, the other 2^70
are these inverse?
no, but suppose we shift out 30. then we get 2^0 and 2^60
maybe from here down we need two multiplications. but we can do it with a common shift of 50. then 20 is left for one.
first lets just try full multiplications.

to make this work we need to multiply by powers of 2. 
how high can we go with H?




0: 2^5
    0: 2^10
1: 2^15
        0: 2^20
2: 2^25
    1: 2^30
3: 2^35
            0: 2^40
4: 2^45
    2: 2^50
5: 2^55
        1: 2^60
6: 2^65
    3: 2^70
7: 2^75

http://dsp-book.narod.ru/FFTBB/0270_PDF_C03.pdf

those that differ by 40 are inverses, and go together.

maybe it's always of the form X^{n/m}+-zeta^{n/m}
this is worth documenting. when does this occur?


apparently you can lay the data out in a grid, then do FFTs in each dimension. of course.

when doing an FFT and doing all roots of unity, maybe we can fit in another layer.
actually there we can just do size 80 FFT, whereas here w're doing size 8 CRT.

first suppose using the full n ROU's generated by 2, a subset is the primitives of those.
then p = Cyclo_n(2)
n need not be a power of 2.

p = Cyclo_n'(2)
here 2 generates n' roots of unity. of those, phi(n') will be primitive. we have
phi(n') = phi(m) = phi(n/ord_n(p))
so we don't need n'=m, we just need phi(n')=phi(m)


what if p is composite?
then we have Phi_n modulo 2 primes. 

we'll do virtual left shifts. 
they will always be by multiples of 5, then 10, then 20.
how to reduce this. review reduction
we split into top and bottom.
bottom becomes low, 32 bits
top becomes mid and high, 8 and 24 bits
we subtract high from low to get diff, 32 bits.
we multiply mid by eps to get prod, 32 bits.
we add diff and prod.

today we want to implement CRT.
we want as little reliance on number system as possible.
so lets use a library to start. 
we implement on CPU.

ring n=2^1024.
p = H.
take an element, split in two, multiply by 4th ROU

maybe we could do it in place, not copying vectors but modifing them.
saves memory, prover just holds the top layer.

now how do we check our answers?
we could try undoing CRT and compare with poly mult.
how to invert CRT.

they're doing inverted form.
first they do m' size 2 FFT's. 
then we'll need to apply eta composition. 

i just don't understand the twiddle matrix.
it's purpose is to transform the output of the p size transform, the input for the m' size transform.
suppose p-size transform is p-tuples of evals at the p'th ROUs. now we want to take p columns, 

so the only reason we got into inverse CRT is to check our answer.
prob better way is long reduction.

ok, lets go on gpu. lets' just work with mod 2^32 first and random ROUs.



we need a clear goal.
lets first confirm what we can do.
i recall roughly we encode commands.
we have pipelines that consist of shaders basically and threadgroup details.
we use an encode to encode commands into a command buffer. we then dispatch the buffer.
each command can be like a blit or compute command.
in blit commands we copy between resources.
in compute commands we invoke a compute pipeline.
in either command we need to set the resources, that is like buffers and textures.
those we can create. 


choices we made:
only use compute shaders, not render or mesh shaders. 
i think we decided against all dynamic libraries.
no function tables.

when should we use a for loop, versus doing all indexing with threads?
suppose there's much more to index than there are threads.
either we use loops in each thread, or i guess it would take multiple pipelines.


can we encapsulate ring elements in a class?
to do so means to restrict it from any thread ops, all must occur in a single thread. 
when it comes to our ring, this may be an option. 
other option is to do it across threads.


maybe they encode pipelines/shaders as modular functions, whereas' I'm programming them as programs. 
in their case, the programming really happens at the encoding level. i wonder about the cost of the context switches. 
maybe we could actually begin not by trying on our own, but by implementing what others have done. 
they often take care of lengths in encoding, leaving the thread just to use its id to perform an operation not aware of global length.


what's the boundary between gpu and cpu? can we think about just what goes on on gpu isolated? think about what resources are in device memory, and how they go to threadgroup memory and get operated on. 
the boundary is commands, those are what we track. 

after we get comfortable with shaders, we'll first have to review libraries and archives and ifnish that. then move on to other metal api stuff. 


using CRT for poly ids:
suppose you want to see f*g+h = 0.
suppose it splits in two. then you are able to test the same equation there. 
now you have two instances of half the size. 
suppose you stop here. then you can batch them together, meaning you compute and commit to the batched quotient.
from the verifier's perspective, the handle to the whole of each part remains a handle to the factors.
randomize the factors, prover commits to quotient, we open.
to open verifier needs to compute eval of the factors, and now multily them together.
seems we have the sumcheck and poly div generalized.
lets indeed try to generalize and formalize.
just think of f*g = h
you always start with arrays f,g of univariate polys. you choose a random input, then reduce to compute the same thing but on the evals.




i like the idea of going through the comp first, holding only the lattice data and the trace, committing to all, then going through again, forgetting about lattice data, and focusing on arithmetization. 




TODO WHEN NEEDED:
fast resource loading (mostly for lookups)
indirect command encoding (mostly for reuse)



i didn't think about how prime is u32, but we're committing in i32.
could we do arithmetic in i32? thats probably what we need. this may change things.
or what if we commit integers using unsigned? so -1 becomes p-1. that would be more complex.

dynamically selecting prime or something. not having any algos fixed, i forget why.


should we do FFTs in parallel?
a round means multiplying
let's actually try to get the architecture right here. 
lets look how others did it. 
but what am I thinking?
we don't want to do it with multiple dispatches, because we want to keep the whole thing in a single threadgroup. our FFTs are small enough for this.
so it certainly must be done with a kingle kernel in a single block.
now in a block, we have many 

an FFT just like sumcheck requires log depth aggregation.
with FFT, you do as many layers as you can inside 



we only know two ways:
1. a new kernel takes care of every round
    poly is in memory. get challenge, each block reads a portion plugging in challenge and write back to mem, but also on reading calcs new poly and aggregates.
2. only do sumcheck inside a threadgroup. issue here is we don't have much memory. one solution is regenerating the data. another issue is many more challenges. 

arithmetization only looks intuitive from the CPU side. maybe we should do it there, and all the irregularity is't a problem. maybe port intermediate results to gpu where commits are made and witness reduction occurs. 


we can't hold that much in shared memory anyway. maybe a solution is to use mutiple kernels, but each reads partial results, recomputes the rest, and writes partial results. 
note how computing the polys the first time (with FFT) is more expensive than the single-evaluation recomputation.



we'll want to use the cpu and gpu both.
smartest to pass from CPU to GPU is probably partial traces. maybe in filtered, sorted orders. 

in general we can say, if there are B units for threadgroups, we can process B times more elements 1/B times the size. biggest issue here is the B times more challenges. we could combine them globally with atomics if we're using a sponge, 


how to do FFT?
so he says can just go to the next round without returning to memory.
the only diff I see is sumcheck must be aggregated. yes, you must return to memory 
couldn't we do this with sumcheck? 

with an FFT, why do you need residency? 

for prefix sum you don't need all block in memory at once. each can calculate it' s result and write to memory. 

if we're going to recompute, might as well do it across blocks. 
i think it's the aggregation that's most costly across blocks. even if don't save all the terms, you have to add them up globally. 

device mem can only hold a few 32 bit words per thread, less than total register files. 

how to make use of execution width?
one way iis using a small field and each handles a copy. but the trace is duplicated. 
so if global memory holds the whole thing to start, 

what's the same position? maybe same 1d index.
maybe it persists for the same pass, which may have multiple kernels. 
how do we set threadgroup memory?

does this achieve the same as resident blocks?
the point was we want a resident blocks. issue was we don't know how many there will be or that they are there at the same time. now I guess it work in the form of threadgroup locations.

if on the GPU, here's what I see:
we basically use each threagroup separately. each one can start at a state and across the group all threads calculate the same trace, but they're all with different randomness. so here it's easy to use a small field. but we don't even need this much parallelism. 
or all threads could be doing global memory reads via imageblocks. 
or we partition the trace by instructions, then a threadgroup reads for an instruction,...
issue is we can't have a block hold much in memory. and if it generates instead it can only use one thread at a time. so if it can only a few kilobytes, do we want a whole proof for it? probably not.
maybe could read over just like memory reads, rest could be generated since it's algebraic. 


if on CPU, here's what I see:
we delegate all crypto at least, to the GPU.
we generate traces and I guess ship the whole thing to the GPU for commit, cuz if we only ship intermediate states it can only delegate one thread to computing it, and also it doesn't know about RAM.
arithmetization happens on the CPU, including all eval reduction, it's just when we finally do the witness reduction that we send over the challenge and it adds. then it can renormalize and commit again.
to keep the GPU busy here one part of CPU is going ahead calculating and sending over data for commit, while another part is behind doing the arithmetization from previous commits. 
and we also pass over a bunch of hash inputs from arithmetization, and the GPU will fully prove these on its own. 

before we g one way or another on arithmetization, CPU or GPU, we'll implement commits on GPU since do those either way.
is commit global? each block reads several elements and commits to them in tree order. 
another possibility is we stream the trace over the gpu for commits, and it get's discarded as it commits, we only need the final output. we'd stream the witness in many parts, basically everything that will be committed.

but remember our logic is algebraic, so must groups diverge?
if not, then they can all start at the top of a function and generate traces. 

First let's just have a big buffer with elements, and each block processes several elements, each group processing one. or maybe each thread processing one. we'll then need to figure out decomposition. output of every block should be a root element.
we'll read the elements to commit in blocks. we commit a block waiting for another. we also iteratively decomposte and commit. 
should a thread or a group commit an element? maybe each thread in a group does it's own, reading an image block. 
this does indeed seem a good starting point, getting it the whole way including streaming over large data sets. 

we maybe need not be worried about lattice size, can be cheap to add more nosie. 

so what's the algo?
first we'll want to read the data. lets use an imageblock.
how to do this?

https://docs.nvidia.com/gameworks/content/developertools/desktop/analysis/report/cudaexperiments/kernellevel/achievedoccupancy.htm

First challenge:
How to read into threadgroup memory? First without imageblocks. 
We can just declare a [[threadgroup(i)]] and write to it, but I'm not sure how large it is. Maybe it points to the beginning of all threadgroup memory. 
we've got our first shader, an inefficient reduction. and we want to profile it. 
i forgot, lets see now. 


recall the goal is to go from non-uniform to uniform.
suppose all the sumchecks were made small. 
actually only the logic is non-uniform and that persists.





We want to implement commits on both sides.
we already did a basic reduction on both sides.

today we implement our commit kernel.
we confirm our algo.

regardless the details each group will commit to one or more elements, and each block will join them together.
we will use imageblocks to read from global. 
but this is an optimization we'd like to benchmark.
maybe first we need to benchmark. 
so we begin with timing counters.









what is the output we expect to compute?
I think it's fully reduced log(S) depth.
yeah, with output that's how it should be.
so we'll also iterate depth log(S)
for each we just apply it. 

so how to split?
well, first we just split in half cuz S=2

suppose S = 4
we're computing index 0 wrong. 
k = 1, i = 0, s = 0
looks like we're computing the high index wrong.
we have
(2 * i + 1) * (S / (1 << (k + 1))) + s
= (2 * 0 + 1) * (4 / (1 << (1 + 1))) + 0
= 4 / (1 << (1 + 1)) = 4 / 4 = 1
this is right as an index of vals.

vals[s] = input[s * (D / S) + t]
vals[0] = input[0+0] = input[0]

Correct: input[D/2] = input[64]
Ours: vals[1] = input[1 * (D / S) + 0] = input[D/4]

oh, we forgot to extend the correct verision.

so what's the naive algo?
we recursive i
at level k, how many decomps do we apply?
at k=0, we apply 1, at k=1, we apply 2, so 2^k
for each we iterate and each is of size D/2^{k+1}
first is of size D/2
so we iterate through this area, offsetingg by this much

k goes from 0 to log(S)-1 (0,1,2)
j goes from 0 to 2^k (1,2,4), counting how many decomps we must apply
each decomp is of whole size D/2^k
l goes from 0 to (D/2^k)/2, counting the indices to fold on
but they need to be adjusted by j. so we need to bump up by j*(D/2^k)

try for k=1
j=0: l from 0 to 64/(1<<(2)) = 16
    hi = j * (D / (1 << k)) + D / (1 << (k + 1)) + l
        = D / (1 << (2)) + l = 64 / 4 = 16 + l
    lo = j * (D / (1 << k)) + l
        = l
j=1, l from 0 to 16
    hi = j * (D / (1 << k)) + D / (1 << (k + 1)) + l
        = D/2 + D/4 + l = 48 + l
    lo = D/2 + l = 32 + l

again, we have to be careful about not overdelegating.
I think we got the algorithm correct. now we need to work it across multiple blocks
for now each block computes for one.
now ever block needs to read into the input at different offsets. 

when G = 2^3=8 we're failing. 
but it works with G = 2^2=4.
fails at 57. seems arbitrary.
how to problem solve this?
can we find any hint where 57 is coming from? its on the 58'th element that it fails, index 57.

we have 8 blocks. in each we have 32 threads. each thread reads 2 values.
so each block should have input 64.
we're failing at an index in just the first block.
it occurs when we enumerate g = 3,4
neither by itself gives the error
oh, blocks are not of size G, but of size D

now what if we make thread size more than warp size?
it shouldn't matter cuz we're not using warps now.

what determines register size? S*T

it can support S = 2^10, T = 2^9, D = 2^19
while it can't support S = 2^5, T = 2^10, D = 2^15
Why?
There are T threads. So the larger the number of threads, the more warps.
Each thread reads S inputs, so the larger S the larger the register space for each.
To support the max threads per threadgroup 2^10, we need S <= 2^4.




















Here we'll consider the performance of just reading and writing. 

SD = 1, TD = 10
    GD = 19: 68
    GD = 18: 31
    GD = 17: 16
    GD = 16: 7
    GD = 15: 4
    GD = 14: 2
SD = 2, TD = 10
    GD = 18: 65
    GD = 17: 32
    GD = 16: 15
    GD = 15: 8
    GD = 14: 4
    GD = 13: 2
SD = 3, TD = 10
    GD = 17: 76
    GD = 16: 32
    GD = 15: 16
    GD = 14: 8
    GD = 13: 4
SD = 4, TD = 10
    GD = 16: 71
    GD = 15: 32
    GD = 14: 15
    GD = 13: 8
SD = 5, TD = 9
    GD = 16: 80
    GD = 15: 33
    GD = 14: 16
    GD = 13: 8
SD = 6, TD = 8
    GD = 16: 66
    GD = 15: 33
    GD = 14: 16
    GD = 13: 8
SD = 7, TD = 7
    GD = 16: 104
    GD = 15: 56
    GD = 14: 26
    GD = 13: 11
SD = 8, TD = 7
    GD = 15: 150
    GD = 14: 70
    GD = 13: 36

What's the pattern?
The theoretical pattern seems to be
27 -> 8
28 -> 16
29 -> 32
30 -> 64
We have not preference on size if this were to hold.

The last two, however, take double time.
27 -> 16
28 -> 32
29 -> 64
30 -> 128

The best we've got is D = 2^30 for time around 64, winner is SD = 2, TD = 10, GD = 18, time 65
But we see that as we increase SD and decrease TD we get worse for large sizes.
But we want larger SD to have less warps to communicate across. 
It's not too bad. so i think we don't have to worry about memory reads. this is the cost of memory. now we see the cost of arithmetic. 

Now we try these same params but with arithmetic.

SD = 1, TD = 10
    GD = 19: 
    GD = 18: 
    GD = 17: 
    GD = 16: 
    GD = 15: 
    GD = 14: 
SD = 2, TD = 10
    GD = 18: 65 / 61
    GD = 17: 32 / 30
    GD = 16: 17 / 15
    GD = 15: 10 / 7
    GD = 14: 4 / 4
    GD = 13: 2 / 2
SD = 3, TD = 10
    GD = 17: 
    GD = 16: 
    GD = 15: 
    GD = 14: 
    GD = 13: 
SD = 4, TD = 10
    GD = 16: 238 | 60
    GD = 15: 125 | 28
    GD = 14: 66 | 14
    GD = 13: 43 | 7
SD = 5, TD = 9 ????
    GD = 16: 
    GD = 15: 288
    GD = 14: 159
    GD = 13: 83
SD = 6, TD = 8
    GD = 16: 280 |
    GD = 15: 142 | 
    GD = 14: 78 |
    GD = 13: 42 |
SD = 7, TD = 7
    GD = 16: 350
    GD = 15: 175
    GD = 14: 87
    GD = 13: 46
SD = 8, TD = 7
    GD = 15: 651
    GD = 14: 364
    GD = 13: 177

we're back, looking at the same shaders, and now getting half the performance.

There the last two are no longer approximately the samee, but now the last about twice as slow.

Maybe we're doing this wrong. 
We want to perform with constant S and D.
I've expanded and it performs the 

Wow, just reading takes like 32/33, whereas with the arithmetic it takes like 27/28, how can that be?
we can't try for the others without expanding. 
well, it's easier to expand them. try for S = 2.
So for S=2, there's little advantage, because without expansion it's already close to reading rate.
the advantage grows with the amount of arithmetic. 
but for now we're not going to expand for higher. 
so we need to figure out why expanding it gives a benefit. shouldn't Metal expand it for us?
maybe we can wait on this, to think about it more.
I realized the reason is it only does the trivial calculations. this why there's loop unrolling. so indeed, we'll have to unwroll our own loops. 
maybe it's simply that it wasn't doing those instructions in parallel but following the loop. so we only unroll the inner two loops.

how do you reach bandwidth?

what is our occupancy?
that' active warps / 32
but how do we know how many warps are active?

oh, right, a core can execute multiple blocks at once. you want many active warps from multiple blocks in a core at once. 

calculate the number of registers per warp.
then divide that by the register file size, to get how many warps can be active at once. 
then divide by number of possible active warps per block... do we know what that is?

unroll more loops and you use more registers, decreasing occupancy. 

simd_shuffle(data, lane_id)
simd_shuffle_and_fill_X(data, filling_data, delta)
simd_shuffle_and_fill_X(data, filling_data, delta, modulo)
simd_shuffle_X(data, delta)
    non-wrapping
simd_shuffle_rotate_X(data, delta)
    wrapping
simd_shuffle_xor(value, mask)

maybe we want to use quads, then after sharing these we have to share among the 8 sections. 

at the moment we don't know how to make use of ballots and logicals.
reductions also don't seem helpful
the way we need to share is all pairs to share need to multiply by the same.
we're just sharing vectors with each other. 
i think we'll restrict to shuffles.

maybe we can use the xor.
we need a single mask. 
we know what it would be: 1 << log(T)-l-1;
note it goes in pairs, just as we need.
but it only works in warps.
which is fine when we're done to having T/2^l <= 32, l >= log(T)-5, eg log(T)=5.
so suppose l+w >= log(T)
would this work now? well bit log(T)-l-1 <= w-1 so yes, that bit is one the w bits. 
so that's perfect for us.
again, can we handle log(T)=5?
S + T + U = 10, then we just need S + U = 5, and I think we can d that.
so we should only need this shuffle operation.
can also try quad xor. 

how are we supposed to get the block from t and r?
actually not from t but from 
i guess s gives us the top bits of i, whereas r, the top bits of tau, give us the bottom bits of i.
why do we need i? in order to get zeta.
ok, we discard the lower l+2 
how many bits? log(T). 
highest bit? log(T)-1.
I think highest bit to discard? log(T)-l-1
remaining number of bits? l.
t is log(T)-l-1 bits, b is 1 bit.
r must then be log(T) - (log(T)-l-1) - 1 = l bits
we discard the lower log(T)-l bits.

i think our algo is basically right, still just powers of 2 and a single zeta.
i feel like waiting to do zetas and primes cuz i want to know the performance without them.
then i need to unroll. 


a major challenge will be moving data in and out.



we need to organize the coef order.
i guess we want to correctly implement what the thread should hold. 
we have the coefs, and we have the roots of unity, we can actually just look those up too.

we stay focused.
every moment we're aware and effective.
right now we need to check the two algos against each other.
we will compute our main result. then we will check against the correct result, by computing the correct result in monomial form, then transforming that form to the thread form.

we're wrong on the upper half of each thread. but i'm not sure which is right.

lets be clear how we're supposed to compute correctly.
we can determine the ordered blocks, and reduce inside each one.
then we need to take these blocks and transform to thread space.
or, we could try to transform threadspace to blockspace.

lets make sure we have the basic reduction correct.
we start with a k. we need to decompose into 2^{k+1} parts.
zeta is a primitive 2^{k+2}'th ROU. 
we need to compute the binary tree.
given an i, we'll retrieve the correct zeta

we're getting the top two blocks wrong.
yay, it was for the same problem of using i rather than 2i.
but now why would it work for the first two blocks?
note it was our algo that was wrong, not the correct one.
why would it work?
it worked for i=0 and i=1,
oh, i see, the second block works because we use the same zeta for every pair of blocks.

seems even to work with U.

in order to reduce to beylong 2^8 blocks, we need non-power of 2 multipliers.
so we need to examine these now.
i'm hoping we can only need 1/8'th as much by multiplying with powers of 2.
indeed, consider some k >= 3. 
we want to split into 2^{k+1} parts.
so we consider zeta a primitive 2^{k+2}'th ROU.
and we'll build it up in binary form across i.
so every tree coming out from one of the 8 top blocks has a set of children with a common parent, which has common exponents (1+i1*2)(1+i2*4)(1+i3*8)
all sets have the same subtree exponents. only the parent exponents differ.
but it's not like we can share them exactly, not multiplicatively. we could compute the subtree exponents, and then exponentiate recursively by (1+0*2) and (1+1*2). basically this will mean raising some to 2^1, 2^2, or 2^3.
why can't we use multiplying by powers of 2 here? that would be multiplication and we need exponentiation.
so either we do this exponentiation, not too bad, or we store them. how many mult? well first take the tree and square it's elements, multiply by itself. then you'll repeat but actually next time you'll need to exponentiate by 4, then next time 8 (i was hoping only repeated squaring)
we'll probably be better to store them.
either way, we need to compute at least one subtree. and the way we do it we compute from the bottom up, so we need to know to what level we want to decompose, to what k+1, then we find PROU for k+2, then we exponentiate it with subtree exponents up to (1+i_{k+1}*2^{k+1}) from (1+i4*2^4)


let focus for now on structured other PROUs
8'th ROU
(w+w^7)^2 = w^2+w^14+2*w^8 = 2
w^2(w^12+1)=w^2(w^4+1)=0
this works for us too
w is a PROU for 8
2^10+2^70
powers of 2 will grow large here. we need to figure out how to reduce


how are we going to do this?
we'll have a D size. then we have multiple in a block and multiple blocks.
for now we can i guess try one per block, and make it large. 

we want to read in multiple at once. how do we do that? we set the type to be in vectors of that size.




wow, how to do codegen?
we'll have functions to add.
like add a for loop.

what's the structure?

what about unrolling? I guess we will do that ourselves in how we generate it.
we can just make primitive insertions. could be just concatnation. 
we'd want to chain these

we need to keep track of variables.
we have overlap in the primitive types
we can implement some collection types, and have copy traits. 
i'm not sure how they do it with 'rust gpu'.
but where as the gen happen?
suppose you've implemented types and you make the code. 
we won't implement types.
but even plain code seems hard.
like declaring scope. 


this isn't working anymore. why half the speed?
really, the original in risc0 is still so slow. 
both for reading and with arithmetic. 
we're consistently getting one order of magnitude slower. 
have no idea why. i guess we forget about, and just optimize amap. 

risc0 seems to not use any loop unrolling or any templates

omg, now it's performing as before... 
even a little better.
no idea what happened.

i want to try reading in while we compute rather than reading all then computing all. 
this means 
we basically got what we wanted. 
note we need to unroll all the loops if we're going to avoid any index aritmetic. best if we only keep u rolled since its addition. 

now we're trying to do decomposition across threads on gpu.
before we identified the relevant threads and calculated on them.
here we gotta go the other way. we are thread centric. so given a thread id we need to know when to use it. 
notice how the index and the first parts of the array id are the same across T/2^{l+1}, and only the arrays vary with t. i think there's a swap for every t. eg with l=0, every two threads swap.
given an l value. we can consider the range of t. and also beta_{l+1}(i,b).
we need every thread to self identify. 

suppose we are only given a thread id tau in T. 

U has nothing to do with decomp.
so why can we decompose log(TU) times?
i think the U should have no affect. and it seems it doesn't, but why?
basically if we're going to decompose with u it needs to happen after cross thread communication within the thread again.
but we won't do that since we'll have ord >= U
what happens if you apply log(T) times? it has no effect.
so 



with TD=10, TS=4, TG=14,
before with just independen threads we got 13.
now we get more like 40 when doing the cross threads, obviously that 10 costs a lot. generates a ton of code, yet luckily metal can still handle it. 
luckly we'll only have like 5 in practice. 
we should calculate the total inner of each one.

in order to start optimizing with real number system and zetas we should streamline the independent code generation.
we should also start having multiple instances per block.
but actually first, without optimization, we need the zetas to start working. I guess we start with native number system. 
so we're making the decision to switch to number system now having a rough idea as it performs without it.

so how do we make the switch to zeta's on the GPU?
we'll have to look them up. or embed them in code. we probably need to try both.
if we look them up, does that mean constant address space?
could it go anywhere else? we could put them in bytes.

we don't know any way to calculate these roots, except by exponentiation, right?

i just don't like loading so many values into memory. not just these but the constants.
which is larger? constants are larger.

maybe only 128 roots.
why?
there are 1024 total, divided by 8 that's 128. then it's twice that for all upper levels. 
but then we only load half of them, so again 128. and then don't need top ones. 

lets review. seems this hardly saves anything. an additive amount. 
oh, i see. it's a multiplicative amount, because each layer has the same amount of arithmetic. we can save 3 layers this way. we have a total of 8 layers.
so maybe worth trying with sqrt{2}


DIT:
inside in recursion you use primitive N/k'th ROUs.
outside you need to evaluate each result on a coset of size k.
you can instead multiply monomials by a representative of the coset, and then eval on the subgroup the k'th ROUs. 
so you can store 1/k of the zetas.

DIF:
outside is eval on the subgroup the k'th ROUs.
on the inside, you want to eval on the cosets. for each you can scale monomials appropriately, then eval on N/k'th ROUs.
here it seems you only need to store 1 representative for each layer, since you end up back with ROUs.
on the other hand, as you go down the layers, your outside grows more distant from unity.

so we basically get it. 
so what about this for CRT?
why can we not apply DIF?
maybe we can. split in two and want to evaluate each on half of the m PROUs.
can we shift one to another? 
i think so. are they cosets? 

yeah you can do this, using the binary decomp.
oh no you can't. it's not multiplication, it's exponentiation.
so we need to see if it's a multiplicative coset.

so I think all there is for us to think about is whether we can apply DIF as we are but with renormalization. 
with renormalization we need fewer zetas, but you more multiplications.
this may or may not be worth it for us.
there's also the option to go to higher radix, especially if we normalize.
not sure if it's an option without normalizing.

that multiplication is expensive. i guess i'ts not normal to do so. 
normal instead is to do as we do, letting it partition away from unity.

so with DIF, what about the higher factors?
well, at the top 3 levels we don't need it, since we're already using powers of 2. 

then on cross thread we can't really do it efficiently since we can swap only two at a time.
or I guess they could swap multiple times before calculating.
can we get a formula to apply higher radix when we want?
just a formula for radix 4 would be great.


I think we'll be fine not to do reduce the zetas, cuz it's small compared to the constants we need to load. 

oh, could we even shift if we wanted? 
primitive ROU's don't form a group.
question is, can we take of the prim N'th ROUs, 

usually we can take N'th ROUs, and multiply by a rep to get 2N'th ROUs.
they also come in pairs with equal squares. 
eg rep could be primitive 2N'th ROU.

can we do the same for prims?
can we take the N'th PROUs, and multiply by a constant? 
consider the N'th PROUs to be in terms of eta = zeta^2.
then the 2N'th PROUs I think are zeta raised to odd exponents. so 
the exponents are a group under multiplication. but they are not a group under addition.

so suppose we jumped two layers at once.



we can't say there's no bit reversal. our outputs are reversed as they're not in an order generated by zeta.


i guess where you try to save zeta with DIF,
that's your twiddle to apply anyway, but you're applying to a large poly so you need to take powers. 


it's likely we'll have more S than we need. cuz we'd rather read in there than U.
i'm wondering if we could begin multiplication before we finish reduction.
No, we will fully reduce using S, then only partially reduce with T. 

S=4,T=5,U=2
this allows for 2 inputs in a single warp.

remember we want to try doing arithmetic while loading. it's more complex loop unrolling. 

what if we use complex fft instead?
you have polys and you evaluate them we don't have enough precision.

we'll need 3*3*3 multiplications. rather than 4+2+1

(x+1)(x-1)

we gotta know where the coefs are.
oh damn, it seems like coefs are still distributed among threads, at most u coefs of each on a single thread.
so again we'll need cross thread communication.
can they exchange information as before? that would be the top and bottom. 
each thread can interact with arbitrary constants. 
so this cross thread communication should shape our Karatsuba algo.
only the middle multiplication and final additions require cross-thread communication. 
so we can communicate as we like. 
we're already setup to communication with top, bottom. but no reason we couldn't instead do odd, even.

damn, do we need to store the whole tree?
yup, fuck. well it's the same size as the input. main issue here is we'll have to write it back out to memory. fuck, and send it back to the host. 
we have a tree. leaves pass through a linear function, and the child is something else that through another linear function matches it.
f(a1,a2) = b, g(c) = b
f(a1',a2') = b', g(c') = b'
c=c' => b=b'
it really shouldn't matter what these linear functions are.
f is taking input in poly form, computing CRT form, and multiplying with constants. g is binary composition. 
suppose we don't hold on to intermediate nodes. prover on CPU gets eval on leaves and folds in half. or first suppose prover folds instances. then suppose we re-normalize. 

how many nodes would we have?
we have 2 to 1 compression. so as many nodes as leaves. how many leaves? suppose in 8 bit leaves. suppose 8 to 1 compression in elements. so 1024*8 = 2^13 bytes as an input. we must caluclate how many inputs. well if we can do 2^30 input bytes, then that's quite a lot, 2^17 inputs.
other option is to increase constants in place of fewer nodes. here is where we'd use fast resource loading. giving each block a different set of constants. while we'd like each block to write back as little as possible, we're limited by how many constants we can hold.
well actually that's not true. at the moment each block reads in its own constants. 

i forgot, do we need to present the intermediate nodes? yeah. do we need to show smallness? no, don't think so. 



another choice for primes would be cyclos 31 and 16, both requiring 2 challenges. both mod 2^11. both primes only 2 terms. 

supppose I evaluate an integer polynomial at N points (a coset of the N'th roots of unity), and wish to interpolate, applying the inverse FFT. how can I quantify the precision needed, like given a floating precision I'll interpolate with what precision, can I always round to nearest integer? 

it seems to me that complex fft isn't a bad option given speed. 


comparing P and f32.

we're getting an error with jut T. and it only appears only for T > 5.
how many threads are allowed? only 32? I thought 1024. 
oh, it's because we're doing subgroup ops. we need to generalize i guess no to having multiple warps per block. or maybe we already have it, but we need to change the ordering.
so right now we have it folding in half. 
i'm not sure if we can properly test until we fix this. we need thread decomp to only happen across warps. this will indeed be the case if we can have multiple elements per block rhater than one large one.let the number of blocks match the number of warps. 

we have S,T,U, with T=32.
then we need W and B to mark warp count and block count.
so how does a thread read into it's element?
sTH+tU+u once accessed it's element.
but how to get there?

we could take thread position in threadgroup for t, and 

now when we set to 32 threads in a warp and 32 warps in a block, and S=2, we get zero's back. but with U>2 we don't. 
if there was only one block

maybe compare reduction on gpu using complex vs prime, including when we loopunroll. 



